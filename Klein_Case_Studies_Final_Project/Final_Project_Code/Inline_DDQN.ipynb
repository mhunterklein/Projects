{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing eagerly?  True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import gym\n",
    "import imageio\n",
    "from collections import deque\n",
    "from skimage.transform import resize\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4*1024)])\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "        \n",
    "tf.compat.v1.enable_eager_execution()\n",
    "tf.keras.backend.clear_session()\n",
    "print('executing eagerly? ',tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessImage(img):\n",
    "    img = np.mean(img,axis=2)\n",
    "    img = img[::2,::2]\n",
    "    return img\n",
    "\n",
    "def make_DDQN():\n",
    "    \n",
    "    Input = keras.layers.Input(shape=(105,80,4))\n",
    "    \n",
    "    Conv1 = keras.layers.Conv2D(32,(8,8),strides=4,activation='relu',\n",
    "                                kernel_initializer=keras.initializers.VarianceScaling(scale=2),\n",
    "                               use_bias=False)(Input)\n",
    "    \n",
    "    Conv2 = keras.layers.Conv2D(64,(4,4),strides=2,activation='relu',\n",
    "                                kernel_initializer=keras.initializers.VarianceScaling(scale=2),\n",
    "                               use_bias=False)(Conv1)\n",
    "    \n",
    "    Conv3 = keras.layers.Conv2D(64,(3,3),strides=1,activation='relu',\n",
    "                                kernel_initializer=keras.initializers.VarianceScaling(scale=2),\n",
    "                               use_bias=False)(Conv2)\n",
    "    \n",
    "    Conv4 = keras.layers.Conv2D(1024,(2,2),strides=1,activation='relu',\n",
    "                               kernel_initializer=keras.initializers.VarianceScaling(scale=2),\n",
    "                               use_bias=False)(Conv3)\n",
    "    \n",
    "    \n",
    "    split_state_values = keras.layers.Flatten()(Conv4)\n",
    "    \n",
    "    split_action_values = keras.layers.Flatten()(Conv4)\n",
    "    \n",
    "    state_values = keras.layers.Dense(1)(split_state_values)\n",
    "    \n",
    "    raw_advantages = keras.layers.Dense(6)(split_action_values)\n",
    "    \n",
    "    advantages = raw_advantages - keras.backend.max(raw_advantages, axis=1, keepdims=True)\n",
    "    \n",
    "    Q_values = state_values + advantages\n",
    "    \n",
    "    model = keras.Model(inputs=[Input],outputs=[Q_values])\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.RMSprop(lr=0.00025,rho=0.95,epsilon=0.01),loss=keras.losses.Huber())\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_Epsilon(current_frame,n_decay_frames,max_epsilon,min_epsilon,exploration_time=50000):\n",
    "    current_frame = current_frame - exploration_time\n",
    "    slope = (min_epsilon - max_epsilon) / n_decay_frames\n",
    "    \n",
    "    return max_epsilon - slope*current_frame\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state,model,eps,env):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        Q_vales = model.predict(state[np.newaxis].astype(np.float32))\n",
    "        action = np.argmax(Q_vals[0])\n",
    "    return action\n",
    "\n",
    "def clip_rewards(reward):\n",
    "    return np.sign(reward)\n",
    "\n",
    "def sample_experiences(batch_size,replay_buffer):\n",
    "    indices = np.random.randint(len(replay_buffer),size=batch_size)\n",
    "    batch = [replay_buffer[index] for index in indices]\n",
    "    \n",
    "    states, actions, rewards, next_states, dones = [\n",
    "        np.array([experience[field_index] for experience in batch])\n",
    "        for field_index in range(5)\n",
    "    ]\n",
    "    return states, actions, rewards, next_states, dones\n",
    "\n",
    "def generate_gif(frame_number, frames_for_gif, reward, path):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            frame_number: Integer, determining the number of the current frame\n",
    "            frames_for_gif: A sequence of (210, 160, 3) frames of an Atari game in RGB\n",
    "            reward: Integer, Total reward of the episode that es ouputted as a gif\n",
    "            path: String, path where gif is saved\n",
    "    \"\"\"\n",
    "    for idx, frame_idx in enumerate(frames_for_gif): \n",
    "        frames_for_gif[idx] = resize(frame_idx, (420, 320, 3), \n",
    "                                     preserve_range=True, order=0).astype(np.uint8)\n",
    "        \n",
    "    imageio.mimsave(f'{path}{\"ATARI_frame_{0}_reward_{1}.gif\".format(frame_number, reward)}', \n",
    "                    frames_for_gif, duration=1/30)\n",
    "    \n",
    "def skip_3_frames(env,action,p_obs):\n",
    "    obs1, r, done, info = env.step(action)\n",
    "    obs2, r, done, info = env.step(action)\n",
    "    obs3, r, done, info = env.step(action)\n",
    "\n",
    "    p_obs1,p_obs2,p_obs3 = preprocessImage(obs1),preprocessImage(obs2),preprocessImage(obs3)\n",
    "    #print(p_obs.shape,p_obs1.shape)\n",
    "    p_obs_k = np.stack([p_obs,p_obs1,p_obs2,p_obs3],axis=2).astype(np.uint8)\n",
    "\n",
    "    return p_obs_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "MAX_FRAMES = 10000000\n",
    "EPS_DECAY_FRAMES = 1000000\n",
    "FREE_EXPLORE_FRAMES = 50000\n",
    "TARGET_UPDATE_FREQUENCY = 10000\n",
    "ENV_NAME = \"PongDeterministic-v4\"\n",
    "N_OUTPUTS = 6\n",
    "REPLAY_LENGTH = 300000\n",
    "MAX_EPISODE_FRAMES = 2000\n",
    "EPS_MIN = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "model = make_DDQN()\n",
    "target = keras.models.clone_model(model)\n",
    "target.set_weights(model.get_weights())\n",
    "current_frame = 0\n",
    "best_score = -100000\n",
    "exp_replay = deque(maxlen=int(REPLAY_LENGTH))\n",
    "optimizer = optimizer=keras.optimizers.RMSprop(lr=0.00025,rho=0.95,epsilon=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  100\n",
      "replay_buffer length  21407\n",
      "current_frame:  21407\n",
      "last_reward:  -0.02336448598130841\n",
      "last_action:  1\n",
      "episode:  200\n",
      "replay_buffer length  42932\n",
      "current_frame:  42932\n",
      "last_reward:  -0.017921146953405017\n",
      "last_action:  3\n",
      "episode:  300\n",
      "replay_buffer length  64383\n",
      "current_frame:  64383\n",
      "last_reward:  -0.030303030303030304\n",
      "last_action:  3\n",
      "episode:  400\n",
      "replay_buffer length  86118\n",
      "current_frame:  86118\n",
      "last_reward:  -0.024691358024691357\n",
      "last_action:  4\n",
      "episode:  500\n",
      "replay_buffer length  107520\n",
      "current_frame:  107520\n",
      "last_reward:  -0.02912621359223301\n",
      "last_action:  3\n",
      "episode:  600\n",
      "replay_buffer length  128803\n",
      "current_frame:  128803\n",
      "last_reward:  -0.02040816326530612\n",
      "last_action:  1\n",
      "episode:  700\n",
      "replay_buffer length  150090\n",
      "current_frame:  150090\n",
      "last_reward:  -0.031413612565445025\n",
      "last_action:  5\n",
      "episode:  800\n",
      "replay_buffer length  171730\n",
      "current_frame:  171730\n",
      "last_reward:  -0.025510204081632654\n",
      "last_action:  3\n",
      "episode:  900\n",
      "replay_buffer length  193358\n",
      "current_frame:  193358\n",
      "last_reward:  -0.031413612565445025\n",
      "last_action:  1\n",
      "episode:  1000\n",
      "replay_buffer length  215065\n",
      "current_frame:  215065\n",
      "last_reward:  -0.024154589371980676\n",
      "last_action:  3\n",
      "episode:  1100\n",
      "replay_buffer length  236773\n",
      "current_frame:  236773\n",
      "last_reward:  -0.024896265560165973\n",
      "last_action:  3\n",
      "episode:  1200\n",
      "replay_buffer length  257999\n",
      "current_frame:  257999\n",
      "last_reward:  -0.02242152466367713\n",
      "last_action:  1\n",
      "episode:  1300\n",
      "replay_buffer length  279460\n",
      "current_frame:  279460\n",
      "last_reward:  -0.030612244897959183\n",
      "last_action:  0\n",
      "episode:  1400\n",
      "replay_buffer length  300000\n",
      "current_frame:  301081\n",
      "last_reward:  -0.029556650246305417\n",
      "last_action:  0\n",
      "episode:  1500\n",
      "replay_buffer length  300000\n",
      "current_frame:  322511\n",
      "last_reward:  -0.028985507246376812\n",
      "last_action:  1\n",
      "episode:  1600\n",
      "replay_buffer length  300000\n",
      "current_frame:  343994\n",
      "last_reward:  -0.022026431718061675\n",
      "last_action:  3\n",
      "episode:  1700\n",
      "replay_buffer length  300000\n",
      "current_frame:  365546\n",
      "last_reward:  -0.031413612565445025\n",
      "last_action:  0\n",
      "episode:  1800\n",
      "replay_buffer length  300000\n",
      "current_frame:  386786\n",
      "last_reward:  -0.025423728813559324\n",
      "last_action:  4\n",
      "episode:  1900\n",
      "replay_buffer length  300000\n",
      "current_frame:  408467\n",
      "last_reward:  -0.02109704641350211\n",
      "last_action:  0\n",
      "episode:  2000\n",
      "replay_buffer length  300000\n",
      "current_frame:  429667\n",
      "last_reward:  -0.030303030303030304\n",
      "last_action:  1\n",
      "episode:  2100\n",
      "replay_buffer length  300000\n",
      "current_frame:  451377\n",
      "last_reward:  -0.031413612565445025\n",
      "last_action:  0\n",
      "episode:  2200\n",
      "replay_buffer length  300000\n",
      "current_frame:  472870\n",
      "last_reward:  -0.02912621359223301\n",
      "last_action:  1\n",
      "episode:  2300\n",
      "replay_buffer length  300000\n",
      "current_frame:  494701\n",
      "last_reward:  -0.02830188679245283\n",
      "last_action:  1\n",
      "episode:  2400\n",
      "replay_buffer length  300000\n",
      "current_frame:  516481\n",
      "last_reward:  -0.029556650246305417\n",
      "last_action:  2\n",
      "episode:  2500\n",
      "replay_buffer length  300000\n",
      "current_frame:  537889\n",
      "last_reward:  -0.024154589371980676\n",
      "last_action:  5\n",
      "episode:  2600\n",
      "replay_buffer length  300000\n",
      "current_frame:  559290\n",
      "last_reward:  -0.01984126984126984\n",
      "last_action:  3\n"
     ]
    }
   ],
   "source": [
    "for episode in range(50000):\n",
    "    \n",
    "    episode_reward = 0\n",
    "    episode_start_frame = current_frame\n",
    "    episode_frames = []\n",
    "    episode_rewards = []\n",
    "    if episode %100 == 0 and episode > 0:\n",
    "        print('episode: ',episode)\n",
    "        print('replay_buffer length ',len(exp_replay))\n",
    "        print('current_frame: ',current_frame)\n",
    "        print('last_reward: ',mean_episode_reward)\n",
    "        print('last_action: ',action)\n",
    "        \n",
    "    state = env.reset()\n",
    "    state = preprocessImage(state)\n",
    "    state = skip_3_frames(env,0,state)\n",
    "    \n",
    "    episode_frames.append(state[:,:,-1])\n",
    "    \n",
    "    for step in range(1000,MAX_EPISODE_FRAMES):\n",
    "        current_frame = current_frame + 1\n",
    "        \n",
    "        #Define Epsilon\n",
    "        if current_frame < FREE_EXPLORE_FRAMES:\n",
    "            epsilon=1\n",
    "        elif current_frame < MAX_FRAMES + FREE_EXPLORE_FRAMES:\n",
    "            epsilon = get_Epsilon(current_frame,EPS_DECAY_FRAMES,1,EPS_MIN)\n",
    "        else:\n",
    "            epsilon = max(get_Epsilon(current_frame,EPS_DECAY_FRAMES,EPS_MIN,0.01),0.01)\n",
    "            \n",
    "        \n",
    "        #Get Action\n",
    "        action = epsilon_greedy_policy(state,model,epsilon,env)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = preprocessImage(next_state)\n",
    "        next_state = skip_3_frames(env,action,next_state)\n",
    "        reward = clip_rewards(reward)\n",
    "        \n",
    "        \n",
    "        #Store experience and frames for gif\n",
    "        exp_replay.append((state,action,reward,next_state,done))\n",
    "        episode_frames.append(next_state[:,:,-1])\n",
    "        episode_rewards.append(reward)\n",
    "        \n",
    "        #Update state\n",
    "        state = next_state\n",
    "        \n",
    "        #Training Step\n",
    "        if current_frame > 100 and current_frame%4 == 0:\n",
    "            experiences = sample_experiences(BATCH_SIZE,exp_replay)\n",
    "            states, actions, rewards, next_states, dones = experiences\n",
    "            \n",
    "            next_Q_vals = model.predict(next_states.astype(np.float32))\n",
    "            best_next_actions = np.argmax(next_Q_vals,axis=1)\n",
    "            \n",
    "            next_mask = tf.one_hot(best_next_actions,6)\n",
    "            next_best_Q_values = keras.backend.sum(target.predict(next_states.astype(np.float32))*next_mask,axis=1)\n",
    "            \n",
    "            target_Q_values = (rewards + (1 - dones)*GAMMA*next_best_Q_values)\n",
    "            \n",
    "            #print('train_on_batch_start')\n",
    "            model.train_on_batch(states,target_Q_values)\n",
    "            #print('train_on_batch_end')\n",
    "        if current_frame%TARGET_UPDATE_FREQUENCY == 0:\n",
    "            target.set_weights(model.get_weights())\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    mean_episode_reward = np.mean(episode_rewards)       \n",
    "    if mean_episode_reward > best_score:\n",
    "        best_weights = model.get_weights()\n",
    "        model.save_weights('DDQN_Hail_Mary.hdf5')\n",
    "        best_score = episode_reward\n",
    "        generate_gif(current_frame,episode_frames,episode_reward,'Best_DDQN')\n",
    "        \n",
    "    if current_frame > MAX_FRAMES:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
