{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments.wrappers import ActionRepeat\n",
    "from tf_agents.environments.atari_preprocessing import AtariPreprocessing\n",
    "from tf_agents.environments.atari_wrappers import FrameStack4\n",
    "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
    "from tf_agents.networks.q_network import QNetwork\n",
    "from tf_agents.agents.dqn.dqn_agent import DqnAgent\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from gym.wrappers import TimeLimit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.enable_v2_behavior()\n",
    "\n",
    "# Set up a virtual display for rendering OpenAI gym environments.\n",
    "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 1000000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 1000  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 80000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 32  # @param {type:\"integer\"}\n",
    "learning_rate = 2.5e-4  # @param {type:\"number\"}\n",
    "log_interval = 200  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hunter/anaconda3/envs/RL/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'tf_agents.environments.atari_preprocessing.AtariPreprocessing'> doesn't implement 'reset' method, which is required for wrappers derived directly from Wrapper. Deprecated default implementation is used.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAADSCAIAAABCR1ywAAACh0lEQVR4nO3bPUoDURhAUSMD2lu4CDdgaZeVWNq6GVeSzjIbcBEp0sfOJsg0EgPODLmeUwXyMw8uHw/mTVZXP3h5ffjpLS7Ias6Qz0+nr/X2/jHDSuZx+Nyc/MztzXrSNVxP+ussTuA4geOGpS483mt/szdfuvFe+5u9+a+Y4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOW+xe9H+4/zw25/3nMRMcJ3CcwHGzPpPF/ExwnMBxw3a3X3oNTMgExwkcJ3CcwHECxwkcJ3CcwHECxw2P93dLr4EJmeA4geMEjhM4TuA4geMEjhM4TuA4geOOgbe7vafvkkxwnMBxAscd/x/s0LDKBMcJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHDcsvQBOO3xuvl/f3qzP+q4JjhM4TuA4geMEjhM4TuA4geMEjhM4TuA4geMEjhM4TuA458EX4Nwz4DETHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHDdsd/ul18CETHCcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECx30Bu1ci6JJ73gAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=160x210 at 0x7FA8A9AD0390>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_name = \"PongNoFrameskip-v4\"\n",
    "env = suite_gym.load(env_name,max_episode_steps = 800,gym_env_wrappers=[AtariPreprocessing, FrameStack4])\n",
    "env.reset()\n",
    "PIL.Image.fromarray(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_layer = keras.layers.Lambda(lambda obs: tf.cast(obs, np.float32) / 255)\n",
    "conv_layer_params = [(32,8,4),(64,4,2),(64,3,1)]\n",
    "fc_layer_params = (512,)\n",
    "\n",
    "q_net = QNetwork(train_env.observation_spec(),\n",
    "                train_env.action_spec(),\n",
    "                preprocessing_layers=preprocessing_layer,\n",
    "                conv_layer_params=conv_layer_params,\n",
    "                fc_layer_params=fc_layer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.Variable(0)\n",
    "update_period = 4\n",
    "optimizer = keras.optimizers.RMSprop(lr=2.5e-4,rho=0.95,momentum=0.0,\n",
    "                                    epsilon=0.00001,centered=True)\n",
    "\n",
    "epsilon_fn = keras.optimizers.schedules.PolynomialDecay(\n",
    "                initial_learning_rate=1.0,\n",
    "                decay_steps = 250000 // update_period,\n",
    "                end_learning_rate=0.01)\n",
    "\n",
    "agent = DqnAgent(train_env.time_step_spec(),\n",
    "                train_env.action_spec(),\n",
    "                q_network=q_net,\n",
    "                optimizer=optimizer,\n",
    "                target_update_period=2000,\n",
    "                td_errors_loss_fn = keras.losses.Huber(reduction='none'),\n",
    "                gamma=0.99,\n",
    "                train_step_counter=train_step,\n",
    "                epsilon_greedy=lambda: epsilon_fn(train_step))\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]\n",
    "\n",
    "\n",
    "# See also the metrics module for standard implementations of different metrics.\n",
    "# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-20.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_avg_return(eval_env, random_policy, num_eval_episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trajectory(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), observation=BoundedTensorSpec(shape=(210, 160, 3), dtype=tf.uint8, name='observation', minimum=array(0, dtype=uint8), maximum=array(255, dtype=uint8)), action=BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(5)), policy_info=(), next_step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.collect_data_spec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('step_type',\n",
       " 'observation',\n",
       " 'action',\n",
       " 'policy_info',\n",
       " 'next_step_type',\n",
       " 'reward',\n",
       " 'discount')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.collect_data_spec._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy, buffer):\n",
    "  time_step = environment.current_time_step()\n",
    "  action_step = policy.action(time_step)\n",
    "  next_time_step = environment.step(action_step.action)\n",
    "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "  # Add trajectory to the replay buffer\n",
    "  buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "  for _ in range(steps):\n",
    "    collect_step(env, policy, buffer)\n",
    "\n",
    "collect_data(train_env, random_policy, replay_buffer, steps=800)\n",
    "\n",
    "# This loop is so common in RL, that we provide standard implementations. \n",
    "# For more details see the drivers module.\n",
    "# https://github.com/tensorflow/agents/blob/master/tf_agents/docs/python/tf_agents/drivers.md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (Trajectory(step_type=(32, 2), observation=(32, 2, 210, 160, 3), action=(32, 2), policy_info=(), next_step_type=(32, 2), reward=(32, 2), discount=(32, 2)), BufferInfo(ids=(32, 2), probabilities=(32,))), types: (Trajectory(step_type=tf.int32, observation=tf.uint8, action=tf.int64, policy_info=(), next_step_type=tf.int32, reward=tf.float32, discount=tf.float32), BufferInfo(ids=tf.int64, probabilities=tf.float32))>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7fa8980a8250>\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(dataset)\n",
    "\n",
    "print(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 200: loss = 7.507310510845855e-05\n",
      "step = 400: loss = 0.0005617969436571002\n",
      "step = 600: loss = 0.0004819167370442301\n",
      "step = 800: loss = 0.00043140698107890785\n",
      "step = 1000: loss = 0.006932405289262533\n",
      "step = 1000: Average Return = -19.0\n",
      "step = 1200: loss = 0.0016136913327500224\n",
      "step = 1400: loss = 0.0016825480852276087\n",
      "step = 1600: loss = 0.00026422744849696755\n",
      "step = 1800: loss = 0.0008955611847341061\n",
      "step = 2000: loss = 0.00040434260154142976\n",
      "step = 2000: Average Return = -20.0\n",
      "step = 2200: loss = 0.0002815502230077982\n",
      "step = 2400: loss = 0.00024033701629377902\n",
      "step = 2600: loss = 0.00011516117956489325\n",
      "step = 2800: loss = 0.003458414226770401\n",
      "step = 3000: loss = 0.0002733294968493283\n",
      "step = 3000: Average Return = -20.0\n",
      "step = 3200: loss = 0.0002873480843845755\n",
      "step = 3400: loss = 0.0002381534141022712\n",
      "step = 3600: loss = 0.0007744015892967582\n",
      "step = 3800: loss = 0.00029034592444077134\n",
      "step = 4000: loss = 0.00029683700995519757\n",
      "step = 4000: Average Return = -21.0\n",
      "step = 4200: loss = 0.00023755694564897567\n",
      "step = 4400: loss = 0.0003722886322066188\n",
      "step = 4600: loss = 0.0002737418399192393\n",
      "step = 4800: loss = 0.00014684179041069\n",
      "step = 5000: loss = 0.0008003448019735515\n",
      "step = 5000: Average Return = -21.0\n",
      "step = 5200: loss = 0.000828444492071867\n",
      "step = 5400: loss = 0.00014036367065273225\n",
      "step = 5600: loss = 0.007126993034034967\n",
      "step = 5800: loss = 0.009690041653811932\n",
      "step = 6000: loss = 0.0003296457580290735\n",
      "step = 6000: Average Return = -18.0\n",
      "step = 6200: loss = 0.0003789408365264535\n",
      "step = 6400: loss = 0.0002352258889004588\n",
      "step = 6600: loss = 0.00023596554819960147\n",
      "step = 6800: loss = 0.0001206976012326777\n",
      "step = 7000: loss = 0.0001139303931267932\n",
      "step = 7000: Average Return = -21.0\n",
      "step = 7200: loss = 0.0004728575295303017\n",
      "step = 7400: loss = 0.00035994115751236677\n",
      "step = 7600: loss = 0.00044854593579657376\n",
      "step = 7800: loss = 0.007888458669185638\n",
      "step = 8000: loss = 0.0001750319206621498\n",
      "step = 8000: Average Return = -21.0\n",
      "step = 8200: loss = 0.0003671977901831269\n",
      "step = 8400: loss = 0.0003186641843058169\n",
      "step = 8600: loss = 0.0005406314157880843\n",
      "step = 8800: loss = 0.0011319945333525538\n",
      "step = 9000: loss = 0.00038966850843280554\n",
      "step = 9000: Average Return = -21.0\n",
      "step = 9200: loss = 0.00029655464459210634\n",
      "step = 9400: loss = 0.00023851494188420475\n",
      "step = 9600: loss = 0.00018078365246765316\n",
      "step = 9800: loss = 0.000412891706218943\n",
      "step = 10000: loss = 0.0011980715207755566\n",
      "step = 10000: Average Return = -21.0\n",
      "step = 10200: loss = 0.00017140245472546667\n",
      "step = 10400: loss = 0.0001571898756083101\n",
      "step = 10600: loss = 0.0011359532363712788\n",
      "step = 10800: loss = 0.001270367531105876\n",
      "step = 11000: loss = 0.0030207205563783646\n",
      "step = 11000: Average Return = -21.0\n",
      "step = 11200: loss = 0.00014373073645401746\n",
      "step = 11400: loss = 0.0002042015257757157\n",
      "step = 11600: loss = 0.00014564039884135127\n",
      "step = 11800: loss = 0.0030079542193561792\n",
      "step = 12000: loss = 0.00020303978817537427\n",
      "step = 12000: Average Return = -20.0\n",
      "step = 12200: loss = 0.0006529010133817792\n",
      "step = 12400: loss = 0.00024872436188161373\n",
      "step = 12600: loss = 0.0002104129089275375\n",
      "step = 12800: loss = 0.00010037323227152228\n",
      "step = 13000: loss = 7.05272177583538e-05\n",
      "step = 13000: Average Return = -21.0\n",
      "step = 13200: loss = 0.00016823242185637355\n",
      "step = 13400: loss = 0.000416157825384289\n",
      "step = 13600: loss = 0.00028647363069467247\n",
      "step = 13800: loss = 0.000338704907335341\n",
      "step = 14000: loss = 0.00028330011991783977\n",
      "step = 14000: Average Return = -20.0\n",
      "step = 14200: loss = 0.0007948334095999599\n",
      "step = 14400: loss = 0.00016749720089137554\n",
      "step = 14600: loss = 0.00039410253521054983\n",
      "step = 14800: loss = 0.000360967154847458\n",
      "step = 15000: loss = 8.458548109047115e-05\n",
      "step = 15000: Average Return = -21.0\n",
      "step = 15200: loss = 0.0034435116685926914\n",
      "step = 15400: loss = 0.0003896220587193966\n",
      "step = 15600: loss = 0.0037662347313016653\n",
      "step = 15800: loss = 0.0005116682150401175\n",
      "step = 16000: loss = 0.0017758393660187721\n",
      "step = 16000: Average Return = -21.0\n",
      "step = 16200: loss = 0.00899762287735939\n",
      "step = 16400: loss = 0.0007366821519099176\n",
      "step = 16600: loss = 0.0019572305027395487\n",
      "step = 16800: loss = 0.00021754483168479055\n",
      "step = 17000: loss = 0.00033929289202205837\n",
      "step = 17000: Average Return = -21.0\n",
      "step = 17200: loss = 0.00019057531608268619\n",
      "step = 17400: loss = 0.0002973653026856482\n",
      "step = 17600: loss = 0.0003507151559460908\n",
      "step = 17800: loss = 0.000553038262296468\n",
      "step = 18000: loss = 0.0002423819969408214\n",
      "step = 18000: Average Return = -21.0\n",
      "step = 18200: loss = 0.00021142461628187448\n",
      "step = 18400: loss = 0.0001498524798080325\n",
      "step = 18600: loss = 0.00025507272221148014\n",
      "step = 18800: loss = 0.0007109982543624938\n",
      "step = 19000: loss = 0.0005471452022902668\n",
      "step = 19000: Average Return = -21.0\n",
      "step = 19200: loss = 0.0007163372356444597\n",
      "step = 19400: loss = 0.0001980403030756861\n",
      "step = 19600: loss = 0.00013052912254352123\n",
      "step = 19800: loss = 0.00047860058839432895\n",
      "step = 20000: loss = 0.0002914572833105922\n",
      "step = 20000: Average Return = -20.0\n",
      "step = 20200: loss = 0.00035504664992913604\n",
      "step = 20400: loss = 0.003799318103119731\n",
      "step = 20600: loss = 0.00016785353363957256\n",
      "step = 20800: loss = 0.0020379479974508286\n",
      "step = 21000: loss = 0.00031286090961657465\n",
      "step = 21000: Average Return = -19.0\n",
      "step = 21200: loss = 0.0007320836884900928\n",
      "step = 21400: loss = 0.009347996674478054\n",
      "step = 21600: loss = 0.0015283378306776285\n",
      "step = 21800: loss = 0.00032133905915543437\n",
      "step = 22000: loss = 0.0004090534639544785\n",
      "step = 22000: Average Return = -19.0\n",
      "step = 22200: loss = 0.0006923029432073236\n",
      "step = 22400: loss = 0.0009489117655903101\n",
      "step = 22600: loss = 0.003468419425189495\n",
      "step = 22800: loss = 0.00020124179718550295\n",
      "step = 23000: loss = 0.000480505550513044\n",
      "step = 23000: Average Return = -21.0\n",
      "step = 23200: loss = 0.000406621111324057\n",
      "step = 23400: loss = 0.00024422319256700575\n",
      "step = 23600: loss = 0.008183283731341362\n",
      "step = 23800: loss = 0.001206578454002738\n",
      "step = 24000: loss = 0.00026800387422554195\n",
      "step = 24000: Average Return = -21.0\n",
      "step = 24200: loss = 0.0007641642587259412\n",
      "step = 24400: loss = 0.00037041291943751276\n",
      "step = 24600: loss = 0.0001849806576501578\n",
      "step = 24800: loss = 0.00024405165459029377\n",
      "step = 25000: loss = 0.00018214600277133286\n",
      "step = 25000: Average Return = -21.0\n",
      "step = 25200: loss = 0.0003467354108579457\n",
      "step = 25400: loss = 0.00024580673198215663\n",
      "step = 25600: loss = 0.00042509500053711236\n",
      "step = 25800: loss = 0.006076673977077007\n",
      "step = 26000: loss = 0.00025584775721654296\n",
      "step = 26000: Average Return = -21.0\n",
      "step = 26200: loss = 0.001329388003796339\n",
      "step = 26400: loss = 0.00026712880935519934\n",
      "step = 26600: loss = 0.0005976288812234998\n",
      "step = 26800: loss = 0.0015870585339143872\n",
      "step = 27000: loss = 0.0005953037180006504\n",
      "step = 27000: Average Return = -21.0\n",
      "step = 27200: loss = 0.0002890752220991999\n",
      "step = 27400: loss = 0.0007606144063174725\n",
      "step = 27600: loss = 0.006982688792049885\n",
      "step = 27800: loss = 0.00048398779472336173\n",
      "step = 28000: loss = 0.0007341572199948132\n",
      "step = 28000: Average Return = -21.0\n",
      "step = 28200: loss = 0.0003940689202863723\n",
      "step = 28400: loss = 0.005402779672294855\n",
      "step = 28600: loss = 0.0003257873759139329\n",
      "step = 28800: loss = 0.0014217817224562168\n",
      "step = 29000: loss = 0.00022661243565380573\n",
      "step = 29000: Average Return = -21.0\n",
      "step = 29200: loss = 0.0002652817056514323\n",
      "step = 29400: loss = 0.0001748529903125018\n",
      "step = 29600: loss = 0.00026200938737019897\n",
      "step = 29800: loss = 0.0002644333872012794\n",
      "step = 30000: loss = 0.00020826756372116506\n",
      "step = 30000: Average Return = -21.0\n",
      "step = 30200: loss = 0.0005311356508173048\n",
      "step = 30400: loss = 0.0006339632091112435\n",
      "step = 30600: loss = 0.0005678979796357453\n",
      "step = 30800: loss = 0.00021741833188571036\n",
      "step = 31000: loss = 0.00028268288588151336\n",
      "step = 31000: Average Return = -21.0\n",
      "step = 31200: loss = 0.0008251238032244146\n",
      "step = 31400: loss = 0.00026393908774480224\n",
      "step = 31600: loss = 0.000139936018968001\n",
      "step = 31800: loss = 0.00034132905420847237\n",
      "step = 32000: loss = 0.00023310758115258068\n",
      "step = 32000: Average Return = -21.0\n",
      "step = 32200: loss = 0.00023586495080962777\n",
      "step = 32400: loss = 0.000380440877052024\n",
      "step = 32600: loss = 0.0001758803555276245\n",
      "step = 32800: loss = 0.00015060229634400457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 33000: loss = 0.0003219411591999233\n",
      "step = 33000: Average Return = -21.0\n",
      "step = 33200: loss = 0.00039836709038354456\n",
      "step = 33400: loss = 0.0003528422676026821\n",
      "step = 33600: loss = 0.01672813668847084\n",
      "step = 33800: loss = 0.0001800462487153709\n",
      "step = 34000: loss = 0.0001529402652522549\n",
      "step = 34000: Average Return = -21.0\n",
      "step = 34200: loss = 0.0014992649666965008\n",
      "step = 34400: loss = 0.00018859829287976027\n",
      "step = 34600: loss = 0.0004210616461932659\n",
      "step = 34800: loss = 0.004745563492178917\n",
      "step = 35000: loss = 0.0002268816315336153\n",
      "step = 35000: Average Return = -21.0\n",
      "step = 35200: loss = 0.0002186416822951287\n",
      "step = 35400: loss = 0.0006405994645319879\n",
      "step = 35600: loss = 0.0002664053754415363\n",
      "step = 35800: loss = 0.0003198066260665655\n",
      "step = 36000: loss = 0.00013451880658976734\n",
      "step = 36000: Average Return = -20.0\n",
      "step = 36200: loss = 0.0021787399891763926\n",
      "step = 36400: loss = 0.00055528967641294\n",
      "step = 36600: loss = 0.0001886667450889945\n",
      "step = 36800: loss = 0.0012347219744697213\n",
      "step = 37000: loss = 0.00024676689645275474\n",
      "step = 37000: Average Return = -21.0\n",
      "step = 37200: loss = 0.0004397989541757852\n",
      "step = 37400: loss = 0.00035967706935480237\n",
      "step = 37600: loss = 0.001113681704737246\n",
      "step = 37800: loss = 0.0005054415087215602\n",
      "step = 38000: loss = 0.0004717957926914096\n",
      "step = 38000: Average Return = -21.0\n",
      "step = 38200: loss = 0.0005101689603179693\n",
      "step = 38400: loss = 0.003980335779488087\n",
      "step = 38600: loss = 0.0007032108260318637\n",
      "step = 38800: loss = 0.00022961186186876148\n",
      "step = 39000: loss = 0.0005203228211030364\n",
      "step = 39000: Average Return = -21.0\n",
      "step = 39200: loss = 0.005325092002749443\n",
      "step = 39400: loss = 0.0002380025980528444\n",
      "step = 39600: loss = 0.0004593307967297733\n",
      "step = 39800: loss = 0.0004036797909066081\n",
      "step = 40000: loss = 0.0007661108975298703\n",
      "step = 40000: Average Return = -20.0\n",
      "step = 40200: loss = 0.0006422154838219285\n",
      "step = 40400: loss = 0.00018189227557741106\n",
      "step = 40600: loss = 0.0003260635130573064\n",
      "step = 40800: loss = 0.0001692250370979309\n",
      "step = 41000: loss = 0.0011483549606055021\n",
      "step = 41000: Average Return = -21.0\n",
      "step = 41200: loss = 0.0003743296838365495\n",
      "step = 41400: loss = 0.00025259610265493393\n",
      "step = 41600: loss = 0.0003371305647306144\n",
      "step = 41800: loss = 0.0004712716327048838\n",
      "step = 42000: loss = 0.0006181207718327641\n",
      "step = 42000: Average Return = -21.0\n",
      "step = 42200: loss = 0.0007663422147743404\n",
      "step = 42400: loss = 0.0010093675227835774\n",
      "step = 42600: loss = 0.0004197952803224325\n",
      "step = 42800: loss = 0.0007730686338618398\n",
      "step = 43000: loss = 0.00017821736400946975\n",
      "step = 43000: Average Return = -21.0\n",
      "step = 43200: loss = 0.0003169550618622452\n",
      "step = 43400: loss = 0.0011392836458981037\n",
      "step = 43600: loss = 0.0003807798493653536\n",
      "step = 43800: loss = 0.0009088633814826608\n",
      "step = 44000: loss = 0.0003735724603757262\n",
      "step = 44000: Average Return = -21.0\n",
      "step = 44200: loss = 0.0003892035747412592\n",
      "step = 44400: loss = 0.010341431945562363\n",
      "step = 44600: loss = 0.00021952064707875252\n",
      "step = 44800: loss = 0.00019663361308630556\n",
      "step = 45000: loss = 0.00026452215388417244\n",
      "step = 45000: Average Return = -21.0\n",
      "step = 45200: loss = 0.0005838465876877308\n",
      "step = 45400: loss = 0.0007128831348381937\n",
      "step = 45600: loss = 0.003631667932495475\n",
      "step = 45800: loss = 0.0008251179242506623\n",
      "step = 46000: loss = 0.0004910099669359624\n",
      "step = 46000: Average Return = -19.0\n",
      "step = 46200: loss = 0.0021655363962054253\n",
      "step = 46400: loss = 0.0003430379438214004\n",
      "step = 46600: loss = 0.00016953377053141594\n",
      "step = 46800: loss = 0.00028774538077414036\n",
      "step = 47000: loss = 0.00033321575028821826\n",
      "step = 47000: Average Return = -21.0\n",
      "step = 47200: loss = 0.0016786649357527494\n",
      "step = 47400: loss = 0.0003230982110835612\n",
      "step = 47600: loss = 0.0005099387490190566\n",
      "step = 47800: loss = 0.0005673117702826858\n",
      "step = 48000: loss = 0.001023010932840407\n",
      "step = 48000: Average Return = -21.0\n",
      "step = 48200: loss = 0.0015554516576230526\n",
      "step = 48400: loss = 0.001081180525943637\n",
      "step = 48600: loss = 0.0010463842190802097\n",
      "step = 48800: loss = 0.000188743433682248\n",
      "step = 49000: loss = 0.00032587922760285437\n",
      "step = 49000: Average Return = -20.0\n",
      "step = 49200: loss = 0.0005210291710682213\n",
      "step = 49400: loss = 0.00024545553606003523\n",
      "step = 49600: loss = 0.0003712587640620768\n",
      "step = 49800: loss = 0.00022694403014611453\n",
      "step = 50000: loss = 0.0003092434781137854\n",
      "step = 50000: Average Return = -21.0\n",
      "step = 50200: loss = 0.003031815867871046\n",
      "step = 50400: loss = 0.00033813720801845193\n",
      "step = 50600: loss = 0.0018340210663154721\n",
      "step = 50800: loss = 0.0009763576090335846\n",
      "step = 51000: loss = 0.0003733042103704065\n",
      "step = 51000: Average Return = -20.0\n",
      "step = 51200: loss = 0.0005613040993921459\n",
      "step = 51400: loss = 0.00041252325172536075\n",
      "step = 51600: loss = 0.00034614375908859074\n",
      "step = 51800: loss = 0.00040385776082985103\n",
      "step = 52000: loss = 0.0009053749963641167\n",
      "step = 52000: Average Return = -20.0\n",
      "step = 52200: loss = 0.0002608266659080982\n",
      "step = 52400: loss = 0.0017906059511005878\n",
      "step = 52600: loss = 0.0016324297757819295\n",
      "step = 52800: loss = 0.00048272963613271713\n",
      "step = 53000: loss = 0.00028477812884375453\n",
      "step = 53000: Average Return = -20.0\n",
      "step = 53200: loss = 0.00027277524350211024\n",
      "step = 53400: loss = 0.000766435288824141\n",
      "step = 53600: loss = 0.0008467078441753983\n",
      "step = 53800: loss = 0.00036523115704767406\n",
      "step = 54000: loss = 0.0005677646258845925\n",
      "step = 54000: Average Return = -21.0\n",
      "step = 54200: loss = 0.00034101351047866046\n",
      "step = 54400: loss = 0.0002751402207650244\n",
      "step = 54600: loss = 0.0005376591579988599\n",
      "step = 54800: loss = 0.014067928306758404\n",
      "step = 55000: loss = 0.0005027642473578453\n",
      "step = 55000: Average Return = -21.0\n",
      "step = 55200: loss = 0.0005096025997772813\n",
      "step = 55400: loss = 0.0002244830538984388\n",
      "step = 55600: loss = 0.0005187460919842124\n",
      "step = 55800: loss = 0.0003845968749374151\n",
      "step = 56000: loss = 0.0002359191421419382\n",
      "step = 56000: Average Return = -21.0\n",
      "step = 56200: loss = 0.0006621354841627181\n",
      "step = 56400: loss = 0.00021488734637387097\n",
      "step = 56600: loss = 0.00047696128604002297\n",
      "step = 56800: loss = 0.00036171451210975647\n",
      "step = 57000: loss = 0.0002989610657095909\n",
      "step = 57000: Average Return = -20.0\n",
      "step = 57200: loss = 0.0061136893928050995\n",
      "step = 57400: loss = 0.0011958838440477848\n",
      "step = 57600: loss = 0.0012510104570537806\n",
      "step = 57800: loss = 0.0010804301127791405\n",
      "step = 58000: loss = 0.002539166482165456\n",
      "step = 58000: Average Return = -21.0\n",
      "step = 58200: loss = 0.00024982268223538995\n",
      "step = 58400: loss = 0.009906800463795662\n",
      "step = 58600: loss = 0.0006872190278954804\n",
      "step = 58800: loss = 0.00028387928614392877\n",
      "step = 59000: loss = 0.0007295201066881418\n",
      "step = 59000: Average Return = -21.0\n",
      "step = 59200: loss = 0.0005893858615309\n",
      "step = 59400: loss = 0.0003995609877165407\n",
      "step = 59600: loss = 0.00038791593397036195\n",
      "step = 59800: loss = 0.0003399834386073053\n",
      "step = 60000: loss = 0.0002616672427393496\n",
      "step = 60000: Average Return = -21.0\n",
      "step = 60200: loss = 0.0008117079851217568\n",
      "step = 60400: loss = 0.00032474310137331486\n",
      "step = 60600: loss = 0.001104589900933206\n",
      "step = 60800: loss = 0.0003891942324116826\n",
      "step = 61000: loss = 0.00041086715646088123\n",
      "step = 61000: Average Return = -21.0\n",
      "step = 61200: loss = 0.003893659682944417\n",
      "step = 61400: loss = 0.001631254912354052\n",
      "step = 61600: loss = 0.0007343193283304572\n",
      "step = 61800: loss = 0.0012289199512451887\n",
      "step = 62000: loss = 0.00048721383791416883\n",
      "step = 62000: Average Return = -20.0\n",
      "step = 62200: loss = 0.0017407636623829603\n",
      "step = 62400: loss = 0.001155198086053133\n",
      "step = 62600: loss = 0.009237801656126976\n",
      "step = 62800: loss = 0.0002986730542033911\n",
      "step = 63000: loss = 0.0006495289853774011\n",
      "step = 63000: Average Return = -21.0\n",
      "step = 63200: loss = 0.0006097097066231072\n",
      "step = 63400: loss = 0.0004029291740152985\n",
      "step = 63600: loss = 0.0007352505344897509\n",
      "step = 63800: loss = 0.0012020261492580175\n",
      "step = 64000: loss = 0.005514110904186964\n",
      "step = 64000: Average Return = -21.0\n",
      "step = 64200: loss = 0.0003715464845299721\n",
      "step = 64400: loss = 0.0004008331161458045\n",
      "step = 64600: loss = 0.0002507777826394886\n",
      "step = 64800: loss = 0.0006883417372591794\n",
      "step = 65000: loss = 0.00040556213934905827\n",
      "step = 65000: Average Return = -21.0\n",
      "step = 65200: loss = 0.00042869639582931995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 65400: loss = 0.0014716548612341285\n",
      "step = 65600: loss = 0.0005944047588855028\n",
      "step = 65800: loss = 0.0001477760379202664\n",
      "step = 66000: loss = 0.0009036294068209827\n",
      "step = 66000: Average Return = -21.0\n",
      "step = 66200: loss = 0.002658517099916935\n",
      "step = 66400: loss = 0.0008589349454268813\n",
      "step = 66600: loss = 0.0006065106717869639\n",
      "step = 66800: loss = 0.00041667724144645035\n",
      "step = 67000: loss = 0.0004318807041272521\n",
      "step = 67000: Average Return = -21.0\n",
      "step = 67200: loss = 0.0007268721819855273\n",
      "step = 67400: loss = 0.0010181446559727192\n",
      "step = 67600: loss = 0.010229600593447685\n",
      "step = 67800: loss = 0.001226135529577732\n",
      "step = 68000: loss = 0.0019766134209930897\n",
      "step = 68000: Average Return = -21.0\n",
      "step = 68200: loss = 0.0004130150191485882\n",
      "step = 68400: loss = 0.0007680294220335782\n",
      "step = 68600: loss = 0.0008318402688018978\n",
      "step = 68800: loss = 0.001394795486703515\n",
      "step = 69000: loss = 0.0005568044725805521\n",
      "step = 69000: Average Return = -21.0\n",
      "step = 69200: loss = 0.0005100885173305869\n",
      "step = 69400: loss = 0.0006861133733764291\n",
      "step = 69600: loss = 0.002016581129282713\n",
      "step = 69800: loss = 0.0006329689058475196\n",
      "step = 70000: loss = 0.00041695754043757915\n",
      "step = 70000: Average Return = -21.0\n",
      "step = 70200: loss = 0.0007183850975707173\n",
      "step = 70400: loss = 0.000670133507810533\n",
      "step = 70600: loss = 0.0011473241029307246\n",
      "step = 70800: loss = 0.00132472044788301\n",
      "step = 71000: loss = 0.00041178701212629676\n",
      "step = 71000: Average Return = -21.0\n",
      "step = 71200: loss = 0.00468931021168828\n",
      "step = 71400: loss = 0.0005362665979191661\n",
      "step = 71600: loss = 0.0004217581881675869\n",
      "step = 71800: loss = 0.0010523989330977201\n",
      "step = 72000: loss = 0.0014194013783708215\n",
      "step = 72000: Average Return = -21.0\n",
      "step = 72200: loss = 0.0004889150150120258\n",
      "step = 72400: loss = 0.0010718322591856122\n",
      "step = 72600: loss = 0.0007255894597619772\n",
      "step = 72800: loss = 0.0007874221773818135\n",
      "step = 73000: loss = 0.0007562165847048163\n",
      "step = 73000: Average Return = -21.0\n",
      "step = 73200: loss = 0.0008406759006902575\n",
      "step = 73400: loss = 0.0005556819378398359\n",
      "step = 73600: loss = 0.000561331573408097\n",
      "step = 73800: loss = 0.0007224812870845199\n",
      "step = 74000: loss = 0.0011312842834740877\n",
      "step = 74000: Average Return = -21.0\n",
      "step = 74200: loss = 0.0019373551476746798\n",
      "step = 74400: loss = 0.0005004812846891582\n",
      "step = 74600: loss = 0.0006543091731145978\n",
      "step = 74800: loss = 0.00034748323378153145\n",
      "step = 75000: loss = 0.0006738230586051941\n",
      "step = 75000: Average Return = -21.0\n",
      "step = 75200: loss = 0.00032350816763937473\n",
      "step = 75400: loss = 0.0009991234401240945\n",
      "step = 75600: loss = 0.013723383657634258\n",
      "step = 75800: loss = 0.00034022971522063017\n",
      "step = 76000: loss = 0.0019069858826696873\n",
      "step = 76000: Average Return = -21.0\n",
      "step = 76200: loss = 0.0018261189106851816\n",
      "step = 76400: loss = 0.0006305350107140839\n",
      "step = 76600: loss = 0.0005582475569099188\n",
      "step = 76800: loss = 0.000963727361522615\n",
      "step = 77000: loss = 0.007012015674263239\n",
      "step = 77000: Average Return = -21.0\n",
      "step = 77200: loss = 0.0013580818194895983\n",
      "step = 77400: loss = 0.0006948505761101842\n",
      "step = 77600: loss = 0.00039747601840645075\n",
      "step = 77800: loss = 0.000321095809340477\n",
      "step = 78000: loss = 0.005527927074581385\n",
      "step = 78000: Average Return = -20.0\n",
      "step = 78200: loss = 0.0009223132510669529\n",
      "step = 78400: loss = 0.0013813173864036798\n",
      "step = 78600: loss = 0.0018590122926980257\n",
      "step = 78800: loss = 0.00041803927160799503\n",
      "step = 79000: loss = 0.0003575129376258701\n",
      "step = 79000: Average Return = -20.0\n",
      "step = 79200: loss = 0.001285857055336237\n",
      "step = 79400: loss = 0.0009515093988738954\n",
      "step = 79600: loss = 0.0012730318121612072\n",
      "step = 79800: loss = 0.0006634622695855796\n",
      "step = 80000: loss = 0.0006520312745124102\n",
      "step = 80000: Average Return = -21.0\n",
      "step = 80200: loss = 0.000844641006551683\n",
      "step = 80400: loss = 0.0015875580720603466\n",
      "step = 80600: loss = 0.0007503851084038615\n",
      "step = 80800: loss = 0.006540610454976559\n",
      "step = 81000: loss = 0.0006266853888519108\n",
      "step = 81000: Average Return = -21.0\n",
      "step = 81200: loss = 0.0012473390670493245\n",
      "step = 81400: loss = 0.0010895866435021162\n",
      "step = 81600: loss = 0.0011217931751161814\n",
      "step = 81800: loss = 0.0005928974132984877\n",
      "step = 82000: loss = 0.0006804668810218573\n",
      "step = 82000: Average Return = -21.0\n",
      "step = 82200: loss = 0.00046576361637562513\n",
      "step = 82400: loss = 0.0007473325240425766\n",
      "step = 82600: loss = 0.00042045838199555874\n",
      "step = 82800: loss = 0.0003234100586269051\n",
      "step = 83000: loss = 0.0016096611507236958\n",
      "step = 83000: Average Return = -21.0\n",
      "step = 83200: loss = 0.0005636578425765038\n",
      "step = 83400: loss = 0.000963423924986273\n",
      "step = 83600: loss = 0.001160970889031887\n",
      "step = 83800: loss = 0.01089352648705244\n",
      "step = 84000: loss = 0.0005740430788137019\n",
      "step = 84000: Average Return = -21.0\n",
      "step = 84200: loss = 0.0006292324396781623\n",
      "step = 84400: loss = 0.002435296541079879\n",
      "step = 84600: loss = 0.000376919808331877\n",
      "step = 84800: loss = 0.0018695271573960781\n",
      "step = 85000: loss = 0.0005688914097845554\n",
      "step = 85000: Average Return = -21.0\n",
      "step = 85200: loss = 0.0005321206990629435\n",
      "step = 85400: loss = 0.00036064028972759843\n",
      "step = 85600: loss = 0.0008528397302143276\n",
      "step = 85800: loss = 0.0017760187620297074\n",
      "step = 86000: loss = 0.0025367909111082554\n",
      "step = 86000: Average Return = -21.0\n",
      "step = 86200: loss = 0.0015186541713774204\n",
      "step = 86400: loss = 0.0008452350739389658\n",
      "step = 86600: loss = 0.0009445849573239684\n",
      "step = 86800: loss = 0.0010416398290544748\n",
      "step = 87000: loss = 0.004162807948887348\n",
      "step = 87000: Average Return = -21.0\n",
      "step = 87200: loss = 0.0010988229187205434\n",
      "step = 87400: loss = 0.0006883072201162577\n",
      "step = 87600: loss = 0.0005887543084099889\n",
      "step = 87800: loss = 0.005083196796476841\n",
      "step = 88000: loss = 0.0028610541485249996\n",
      "step = 88000: Average Return = -21.0\n",
      "step = 88200: loss = 0.0029342654161155224\n",
      "step = 88400: loss = 0.00040519447065889835\n",
      "step = 88600: loss = 0.0011840320657938719\n",
      "step = 88800: loss = 0.001095387851819396\n",
      "step = 89000: loss = 0.000531981058884412\n",
      "step = 89000: Average Return = -21.0\n",
      "step = 89200: loss = 0.0010282156290486455\n",
      "step = 89400: loss = 0.001184942782856524\n",
      "step = 89600: loss = 0.0035788025707006454\n",
      "step = 89800: loss = 0.0017722301417961717\n",
      "step = 90000: loss = 0.00024532846873626113\n",
      "step = 90000: Average Return = -21.0\n",
      "step = 90200: loss = 0.012800069525837898\n",
      "step = 90400: loss = 0.0005393394967541099\n",
      "step = 90600: loss = 0.018997715786099434\n",
      "step = 90800: loss = 0.0004127797146793455\n",
      "step = 91000: loss = 0.00031440379098057747\n",
      "step = 91000: Average Return = -21.0\n",
      "step = 91200: loss = 0.00032277789432555437\n",
      "step = 91400: loss = 0.0006945343338884413\n",
      "step = 91600: loss = 0.0007500228239223361\n",
      "step = 91800: loss = 0.00047519596409983933\n",
      "step = 92000: loss = 0.0006676753982901573\n",
      "step = 92000: Average Return = -20.0\n",
      "step = 92200: loss = 0.0007022058707661927\n",
      "step = 92400: loss = 0.0037380526773631573\n",
      "step = 92600: loss = 0.00034592425799928606\n",
      "step = 92800: loss = 0.015842460095882416\n",
      "step = 93000: loss = 0.0005432409234344959\n",
      "step = 93000: Average Return = -21.0\n",
      "step = 93200: loss = 0.0008650823729112744\n",
      "step = 93400: loss = 0.0005945481243543327\n",
      "step = 93600: loss = 0.0003468371869530529\n",
      "step = 93800: loss = 0.0014957827515900135\n",
      "step = 94000: loss = 0.0007484003435820341\n",
      "step = 94000: Average Return = -21.0\n",
      "step = 94200: loss = 0.003619245020672679\n",
      "step = 94400: loss = 0.0029387925751507282\n",
      "step = 94600: loss = 0.0005466848379001021\n",
      "step = 94800: loss = 0.0003479708102531731\n",
      "step = 95000: loss = 0.00037440875894390047\n",
      "step = 95000: Average Return = -20.0\n",
      "step = 95200: loss = 0.0015637329779565334\n",
      "step = 95400: loss = 0.009127270430326462\n",
      "step = 95600: loss = 0.0011582387378439307\n",
      "step = 95800: loss = 0.001109429751522839\n",
      "step = 96000: loss = 0.007490647491067648\n",
      "step = 96000: Average Return = -21.0\n",
      "step = 96200: loss = 0.0005290166009217501\n",
      "step = 96400: loss = 0.000770367740187794\n",
      "step = 96600: loss = 0.00047212603385560215\n",
      "step = 96800: loss = 0.0009793059434741735\n",
      "step = 97000: loss = 0.001364051946438849\n",
      "step = 97000: Average Return = -21.0\n",
      "step = 97200: loss = 0.0029725264757871628\n",
      "step = 97400: loss = 0.00048672553384676576\n",
      "step = 97600: loss = 0.0004581285174936056\n",
      "step = 97800: loss = 0.0007056843605823815\n",
      "step = 98000: loss = 0.00046873214887455106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 98000: Average Return = -21.0\n",
      "step = 98200: loss = 0.0005111374193802476\n",
      "step = 98400: loss = 0.0004308904171921313\n",
      "step = 98600: loss = 0.0010750994551926851\n",
      "step = 98800: loss = 0.0008081676787696779\n",
      "step = 99000: loss = 0.0071191731840372086\n",
      "step = 99000: Average Return = -20.0\n",
      "step = 99200: loss = 0.0005358402268029749\n",
      "step = 99400: loss = 0.007461681496351957\n",
      "step = 99600: loss = 0.001659637549892068\n",
      "step = 99800: loss = 0.0005903308046981692\n",
      "step = 100000: loss = 0.0014789168490096927\n",
      "step = 100000: Average Return = -21.0\n",
      "step = 100200: loss = 0.00027594337007030845\n",
      "step = 100400: loss = 0.00033697750768624246\n",
      "step = 100600: loss = 0.0006166759412735701\n",
      "step = 100800: loss = 0.0005349168786779046\n",
      "step = 101000: loss = 0.0014395052567124367\n",
      "step = 101000: Average Return = -20.0\n",
      "step = 101200: loss = 0.0016722212312743068\n",
      "step = 101400: loss = 0.0010686831083148718\n",
      "step = 101600: loss = 0.000656136660836637\n",
      "step = 101800: loss = 0.0004090660368092358\n",
      "step = 102000: loss = 0.0007809425005689263\n",
      "step = 102000: Average Return = -20.0\n",
      "step = 102200: loss = 0.0011700193863362074\n",
      "step = 102400: loss = 0.002405151491984725\n",
      "step = 102600: loss = 0.0005136844119988382\n",
      "step = 102800: loss = 0.00035796116571873426\n",
      "step = 103000: loss = 0.0024356660433113575\n",
      "step = 103000: Average Return = -21.0\n",
      "step = 103200: loss = 0.0006482433527708054\n",
      "step = 103400: loss = 0.0017605385510250926\n",
      "step = 103600: loss = 0.0012669016141444445\n",
      "step = 103800: loss = 0.0011066114529967308\n",
      "step = 104000: loss = 0.007870208472013474\n",
      "step = 104000: Average Return = -21.0\n",
      "step = 104200: loss = 0.0005870605818927288\n",
      "step = 104400: loss = 0.0011230453383177519\n",
      "step = 104600: loss = 0.001045901793986559\n",
      "step = 104800: loss = 0.0006501880707219243\n",
      "step = 105000: loss = 0.0019332976080477238\n",
      "step = 105000: Average Return = -21.0\n",
      "step = 105200: loss = 0.0035294718109071255\n",
      "step = 105400: loss = 0.0014982400462031364\n",
      "step = 105600: loss = 0.0014847811544314027\n",
      "step = 105800: loss = 0.000491463637445122\n",
      "step = 106000: loss = 0.0006981166079640388\n",
      "step = 106000: Average Return = -20.0\n",
      "step = 106200: loss = 0.0008647501235827804\n",
      "step = 106400: loss = 0.0018679136410355568\n",
      "step = 106600: loss = 0.0005655682180076838\n",
      "step = 106800: loss = 0.0007313787355087698\n",
      "step = 107000: loss = 0.0005241168546490371\n",
      "step = 107000: Average Return = -20.0\n",
      "step = 107200: loss = 0.0009209027630276978\n",
      "step = 107400: loss = 0.00131299311760813\n",
      "step = 107600: loss = 0.0006385663291439414\n",
      "step = 107800: loss = 0.0026227787602692842\n",
      "step = 108000: loss = 0.0023173275403678417\n",
      "step = 108000: Average Return = -21.0\n",
      "step = 108200: loss = 0.001324784941971302\n",
      "step = 108400: loss = 0.0006183654186315835\n",
      "step = 108600: loss = 0.0006316806538961828\n",
      "step = 108800: loss = 0.0008190907537937164\n",
      "step = 109000: loss = 0.0011661144671961665\n",
      "step = 109000: Average Return = -21.0\n",
      "step = 109200: loss = 0.0005682563642039895\n",
      "step = 109400: loss = 0.0016190087189897895\n",
      "step = 109600: loss = 0.002295216079801321\n",
      "step = 109800: loss = 0.001224093372002244\n",
      "step = 110000: loss = 0.0013302424922585487\n",
      "step = 110000: Average Return = -21.0\n",
      "step = 110200: loss = 0.0012922710739076138\n",
      "step = 110400: loss = 0.0015129995299503207\n",
      "step = 110600: loss = 0.0005866643623448908\n",
      "step = 110800: loss = 0.00037826201878488064\n",
      "step = 111000: loss = 0.0016058848705142736\n",
      "step = 111000: Average Return = -21.0\n",
      "step = 111200: loss = 0.0006330393371172249\n",
      "step = 111400: loss = 0.0012077160645276308\n",
      "step = 111600: loss = 0.0018335528438910842\n",
      "step = 111800: loss = 0.004529842175543308\n",
      "step = 112000: loss = 0.0141024524345994\n",
      "step = 112000: Average Return = -20.0\n",
      "step = 112200: loss = 0.000996289076283574\n",
      "step = 112400: loss = 0.0005757054314017296\n",
      "step = 112600: loss = 0.00874090101569891\n",
      "step = 112800: loss = 0.0005037117516621947\n",
      "step = 113000: loss = 0.000972907932009548\n",
      "step = 113000: Average Return = -21.0\n",
      "step = 113200: loss = 0.0042582182213664055\n",
      "step = 113400: loss = 0.0013031770940870047\n",
      "step = 113600: loss = 0.00027552503161132336\n",
      "step = 113800: loss = 0.0007666709134355187\n",
      "step = 114000: loss = 0.000733591616153717\n",
      "step = 114000: Average Return = -21.0\n",
      "step = 114200: loss = 0.0013006168883293867\n",
      "step = 114400: loss = 0.0010884735966101289\n",
      "step = 114600: loss = 0.00048475575749762356\n",
      "step = 114800: loss = 0.0019341743318364024\n",
      "step = 115000: loss = 0.0013299891725182533\n",
      "step = 115000: Average Return = -21.0\n",
      "step = 115200: loss = 0.0007078057969920337\n",
      "step = 115400: loss = 0.01521574892103672\n",
      "step = 115600: loss = 0.0009909998625516891\n",
      "step = 115800: loss = 0.0016804620390757918\n",
      "step = 116000: loss = 0.0005635087727569044\n",
      "step = 116000: Average Return = -21.0\n",
      "step = 116200: loss = 0.0009935330599546432\n",
      "step = 116400: loss = 0.002160881645977497\n",
      "step = 116600: loss = 0.0008246043580584228\n",
      "step = 116800: loss = 0.0012985356152057648\n",
      "step = 117000: loss = 0.0030409633181989193\n",
      "step = 117000: Average Return = -20.0\n",
      "step = 117200: loss = 0.002265566261485219\n",
      "step = 117400: loss = 0.0011404871474951506\n",
      "step = 117600: loss = 0.0009845636086538434\n",
      "step = 117800: loss = 0.0006745667196810246\n",
      "step = 118000: loss = 0.0014178415294736624\n",
      "step = 118000: Average Return = -21.0\n",
      "step = 118200: loss = 0.0007536193006671965\n",
      "step = 118400: loss = 0.0011378303170204163\n",
      "step = 118600: loss = 0.0030403584241867065\n",
      "step = 118800: loss = 0.0006029296200722456\n",
      "step = 119000: loss = 0.010404994711279869\n",
      "step = 119000: Average Return = -21.0\n",
      "step = 119200: loss = 0.0004453700385056436\n",
      "step = 119400: loss = 0.023688478395342827\n",
      "step = 119600: loss = 0.0010354457190260291\n",
      "step = 119800: loss = 0.00029364804504439235\n",
      "step = 120000: loss = 0.010803724639117718\n",
      "step = 120000: Average Return = -21.0\n",
      "step = 120200: loss = 0.006131736561655998\n",
      "step = 120400: loss = 0.001060011563822627\n",
      "step = 120600: loss = 0.0014863356482237577\n",
      "step = 120800: loss = 0.0005204359767958522\n",
      "step = 121000: loss = 0.0006375779630616307\n",
      "step = 121000: Average Return = -21.0\n",
      "step = 121200: loss = 0.0006765288999304175\n",
      "step = 121400: loss = 0.0005103370640426874\n",
      "step = 121600: loss = 0.0021231952123343945\n",
      "step = 121800: loss = 0.003268001601099968\n",
      "step = 122000: loss = 0.011894214898347855\n",
      "step = 122000: Average Return = -21.0\n",
      "step = 122200: loss = 0.0013989309081807733\n",
      "step = 122400: loss = 0.000691081047989428\n",
      "step = 122600: loss = 0.0006026811897754669\n",
      "step = 122800: loss = 0.003393118269741535\n",
      "step = 123000: loss = 0.00034073033020831645\n",
      "step = 123000: Average Return = -21.0\n",
      "step = 123200: loss = 0.0023419649805873632\n",
      "step = 123400: loss = 0.0003239110519643873\n",
      "step = 123600: loss = 0.00046570063568651676\n",
      "step = 123800: loss = 0.0008374109165742993\n",
      "step = 124000: loss = 0.00044503292883746326\n",
      "step = 124000: Average Return = -21.0\n",
      "step = 124200: loss = 0.0010229403851553798\n",
      "step = 124400: loss = 0.0005908095045015216\n",
      "step = 124600: loss = 0.0004717199772130698\n",
      "step = 124800: loss = 0.00248083402402699\n",
      "step = 125000: loss = 0.0004951385199092329\n",
      "step = 125000: Average Return = -21.0\n",
      "step = 125200: loss = 0.002362232655286789\n",
      "step = 125400: loss = 0.0010017999447882175\n",
      "step = 125600: loss = 0.00045171467354521155\n",
      "step = 125800: loss = 0.001456284779123962\n",
      "step = 126000: loss = 0.0004903537919744849\n",
      "step = 126000: Average Return = -21.0\n",
      "step = 126200: loss = 0.001622381154447794\n",
      "step = 126400: loss = 0.0010455482406541705\n",
      "step = 126600: loss = 0.000984002836048603\n",
      "step = 126800: loss = 0.0003240497608203441\n",
      "step = 127000: loss = 0.0006632680888287723\n",
      "step = 127000: Average Return = -21.0\n",
      "step = 127200: loss = 0.0006021196022629738\n",
      "step = 127400: loss = 0.0005078293615952134\n",
      "step = 127600: loss = 0.0011775847524404526\n",
      "step = 127800: loss = 0.0006925964262336493\n",
      "step = 128000: loss = 0.0024268513079732656\n",
      "step = 128000: Average Return = -20.0\n",
      "step = 128200: loss = 0.000481228344142437\n",
      "step = 128400: loss = 0.00048052860074676573\n",
      "step = 128600: loss = 0.0008915096987038851\n",
      "step = 128800: loss = 0.0005989096825942397\n",
      "step = 129000: loss = 0.0024926962796598673\n",
      "step = 129000: Average Return = -21.0\n",
      "step = 129200: loss = 0.0102994991466403\n",
      "step = 129400: loss = 0.003236676100641489\n",
      "step = 129600: loss = 0.0003782591375056654\n",
      "step = 129800: loss = 0.0006050210213288665\n",
      "step = 130000: loss = 0.000632059876807034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 130000: Average Return = -20.0\n",
      "step = 130200: loss = 0.0023657940328121185\n",
      "step = 130400: loss = 0.001782188774086535\n",
      "step = 130600: loss = 0.0009934311965480447\n",
      "step = 130800: loss = 0.00046715419739484787\n",
      "step = 131000: loss = 0.0005348033737391233\n",
      "step = 131000: Average Return = -21.0\n",
      "step = 131200: loss = 0.0005813054158352315\n",
      "step = 131400: loss = 0.0004080795624759048\n",
      "step = 131600: loss = 0.01234182994812727\n",
      "step = 131800: loss = 0.0004223202995490283\n",
      "step = 132000: loss = 0.0005137153202667832\n",
      "step = 132000: Average Return = -20.0\n",
      "step = 132200: loss = 0.0010872399434447289\n",
      "step = 132400: loss = 0.00043438031570985913\n",
      "step = 132600: loss = 0.0003871700610034168\n",
      "step = 132800: loss = 0.0012140879407525063\n",
      "step = 133000: loss = 0.0005876111681573093\n",
      "step = 133000: Average Return = -21.0\n",
      "step = 133200: loss = 0.0011337811592966318\n",
      "step = 133400: loss = 0.00044791019172407687\n",
      "step = 133600: loss = 0.0013732436345890164\n",
      "step = 133800: loss = 0.0027463617734611034\n",
      "step = 134000: loss = 0.0006044377805665135\n",
      "step = 134000: Average Return = -21.0\n",
      "step = 134200: loss = 0.0004063270753249526\n",
      "step = 134400: loss = 0.0024515732657164335\n",
      "step = 134600: loss = 0.0007428638637065887\n",
      "step = 134800: loss = 0.000618488120380789\n",
      "step = 135000: loss = 0.0009005293250083923\n",
      "step = 135000: Average Return = -21.0\n",
      "step = 135200: loss = 0.0005004279082641006\n",
      "step = 135400: loss = 0.0006715729832649231\n",
      "step = 135600: loss = 0.0005180641310289502\n",
      "step = 135800: loss = 0.000595979334320873\n",
      "step = 136000: loss = 0.0009205430978909135\n",
      "step = 136000: Average Return = -21.0\n",
      "step = 136200: loss = 0.0008082386339083314\n",
      "step = 136400: loss = 0.0008632412645965815\n",
      "step = 136600: loss = 0.0021536892745643854\n",
      "step = 136800: loss = 0.0054070306941866875\n",
      "step = 137000: loss = 0.0009075650596059859\n",
      "step = 137000: Average Return = -21.0\n",
      "step = 137200: loss = 0.00104514230042696\n",
      "step = 137400: loss = 0.000978189753368497\n",
      "step = 137600: loss = 0.0015983555931597948\n",
      "step = 137800: loss = 0.00046531588304787874\n",
      "step = 138000: loss = 0.0004110845038667321\n",
      "step = 138000: Average Return = -20.0\n",
      "step = 138200: loss = 0.0005213830154389143\n",
      "step = 138400: loss = 0.0010969436261802912\n",
      "step = 138600: loss = 0.0025563682429492474\n",
      "step = 138800: loss = 0.001163438893854618\n",
      "step = 139000: loss = 0.00037582701770588756\n",
      "step = 139000: Average Return = -21.0\n",
      "step = 139200: loss = 0.004846123047173023\n",
      "step = 139400: loss = 0.0008500246913172305\n",
      "step = 139600: loss = 0.0005992698133923113\n",
      "step = 139800: loss = 0.0009532600524835289\n",
      "step = 140000: loss = 0.0007568422588519752\n",
      "step = 140000: Average Return = -21.0\n",
      "step = 140200: loss = 0.0008638012805022299\n",
      "step = 140400: loss = 0.001755229663103819\n",
      "step = 140600: loss = 0.0006470705848187208\n",
      "step = 140800: loss = 0.00518338568508625\n",
      "step = 141000: loss = 0.0005318683106452227\n",
      "step = 141000: Average Return = -21.0\n",
      "step = 141200: loss = 0.0004973585018888116\n",
      "step = 141400: loss = 0.0027059046551585197\n",
      "step = 141600: loss = 0.0039755357429385185\n",
      "step = 141800: loss = 0.011719617992639542\n",
      "step = 142000: loss = 0.000510272802785039\n",
      "step = 142000: Average Return = -21.0\n",
      "step = 142200: loss = 0.0005506728775799274\n",
      "step = 142400: loss = 0.0008661161409690976\n",
      "step = 142600: loss = 0.00027956871781498194\n",
      "step = 142800: loss = 0.00189413211774081\n",
      "step = 143000: loss = 0.002630629576742649\n",
      "step = 143000: Average Return = -21.0\n",
      "step = 143200: loss = 0.0006727082072757185\n",
      "step = 143400: loss = 0.000777360750362277\n",
      "step = 143600: loss = 0.008149289526045322\n",
      "step = 143800: loss = 0.001148515148088336\n",
      "step = 144000: loss = 0.000919631274882704\n",
      "step = 144000: Average Return = -21.0\n",
      "step = 144200: loss = 0.0003282888210378587\n",
      "step = 144400: loss = 0.00036730949068441987\n",
      "step = 144600: loss = 0.0018733011092990637\n",
      "step = 144800: loss = 0.0005781707004643977\n",
      "step = 145000: loss = 0.0016653507482260466\n",
      "step = 145000: Average Return = -21.0\n",
      "step = 145200: loss = 0.012008430436253548\n",
      "step = 145400: loss = 0.004614526405930519\n",
      "step = 145600: loss = 0.00042039796244353056\n",
      "step = 145800: loss = 0.0008323118672706187\n",
      "step = 146000: loss = 0.0008726117084734142\n",
      "step = 146000: Average Return = -21.0\n",
      "step = 146200: loss = 0.0009224039386026561\n",
      "step = 146400: loss = 0.0007439221953973174\n",
      "step = 146600: loss = 0.004101385362446308\n",
      "step = 146800: loss = 0.0005533434450626373\n",
      "step = 147000: loss = 0.0011812265729531646\n",
      "step = 147000: Average Return = -20.0\n",
      "step = 147200: loss = 0.0016509594861418009\n",
      "step = 147400: loss = 0.005981713533401489\n",
      "step = 147600: loss = 0.021973004564642906\n",
      "step = 147800: loss = 0.0007911778520792723\n",
      "step = 148000: loss = 0.0005681759212166071\n",
      "step = 148000: Average Return = -20.0\n",
      "step = 148200: loss = 0.0004258102271705866\n",
      "step = 148400: loss = 0.0008514525834470987\n",
      "step = 148600: loss = 0.0006212038570083678\n",
      "step = 148800: loss = 0.00038234086241573095\n",
      "step = 149000: loss = 0.0008293022983707488\n",
      "step = 149000: Average Return = -20.0\n",
      "step = 149200: loss = 0.0005320103373378515\n",
      "step = 149400: loss = 0.0009737648069858551\n",
      "step = 149600: loss = 0.0011452314211055636\n",
      "step = 149800: loss = 0.00045267416862770915\n",
      "step = 150000: loss = 0.0031830868683755398\n",
      "step = 150000: Average Return = -17.0\n",
      "step = 150200: loss = 0.0013223346322774887\n",
      "step = 150400: loss = 0.0008687851950526237\n",
      "step = 150600: loss = 0.0005192575044929981\n",
      "step = 150800: loss = 0.01977909542620182\n",
      "step = 151000: loss = 0.003886538092046976\n",
      "step = 151000: Average Return = -20.0\n",
      "step = 151200: loss = 0.0012674399185925722\n",
      "step = 151400: loss = 0.0008650682866573334\n",
      "step = 151600: loss = 0.000810981378890574\n",
      "step = 151800: loss = 0.006100440863519907\n",
      "step = 152000: loss = 0.0037177137564867735\n",
      "step = 152000: Average Return = -21.0\n",
      "step = 152200: loss = 0.00351520674303174\n",
      "step = 152400: loss = 0.0005311993299983442\n",
      "step = 152600: loss = 0.0014241230674088001\n",
      "step = 152800: loss = 0.0015100350137799978\n",
      "step = 153000: loss = 0.0019025609362870455\n",
      "step = 153000: Average Return = -21.0\n",
      "step = 153200: loss = 0.0037540574558079243\n",
      "step = 153400: loss = 0.0035707042552530766\n",
      "step = 153600: loss = 0.0022834278643131256\n",
      "step = 153800: loss = 0.0018353818450123072\n",
      "step = 154000: loss = 0.001176409306935966\n",
      "step = 154000: Average Return = -21.0\n",
      "step = 154200: loss = 0.0009250962175428867\n",
      "step = 154400: loss = 0.012508651241660118\n",
      "step = 154600: loss = 0.0007411951082758605\n",
      "step = 154800: loss = 0.0011431198799982667\n",
      "step = 155000: loss = 0.009457563050091267\n",
      "step = 155000: Average Return = -20.0\n",
      "step = 155200: loss = 0.008177347481250763\n",
      "step = 155400: loss = 0.004409207496792078\n",
      "step = 155600: loss = 0.0007111935410648584\n",
      "step = 155800: loss = 0.022289790213108063\n",
      "step = 156000: loss = 0.0008785671670921147\n",
      "step = 156000: Average Return = -21.0\n",
      "step = 156200: loss = 0.0012344778515398502\n",
      "step = 156400: loss = 0.0005199717124924064\n",
      "step = 156600: loss = 0.006528940517455339\n",
      "step = 156800: loss = 0.0016308615449815989\n",
      "step = 157000: loss = 0.0005862711695954204\n",
      "step = 157000: Average Return = -20.0\n",
      "step = 157200: loss = 0.011082996614277363\n",
      "step = 157400: loss = 0.001117286505177617\n",
      "step = 157600: loss = 0.0007336112321354449\n",
      "step = 157800: loss = 0.002456521615386009\n",
      "step = 158000: loss = 0.0009283479303121567\n",
      "step = 158000: Average Return = -21.0\n",
      "step = 158200: loss = 0.001045754412189126\n",
      "step = 158400: loss = 0.00139963673427701\n",
      "step = 158600: loss = 0.00037943708593957126\n",
      "step = 158800: loss = 0.0005513131618499756\n",
      "step = 159000: loss = 0.0009330829489044845\n",
      "step = 159000: Average Return = -21.0\n",
      "step = 159200: loss = 0.0015789256431162357\n",
      "step = 159400: loss = 0.0006240138318389654\n",
      "step = 159600: loss = 0.0013329205103218555\n",
      "step = 159800: loss = 0.0005461175460368395\n",
      "step = 160000: loss = 0.0007916909526102245\n",
      "step = 160000: Average Return = -20.0\n",
      "step = 160200: loss = 0.00410113949328661\n",
      "step = 160400: loss = 0.0018832306377589703\n",
      "step = 160600: loss = 0.0007812263211235404\n",
      "step = 160800: loss = 0.00044297397835180163\n",
      "step = 161000: loss = 0.0006788083119317889\n",
      "step = 161000: Average Return = -20.0\n",
      "step = 161200: loss = 0.0011829342693090439\n",
      "step = 161400: loss = 0.001170778414234519\n",
      "step = 161600: loss = 0.0030408340971916914\n",
      "step = 161800: loss = 0.002582015935331583\n",
      "step = 162000: loss = 0.0005731978453695774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 162000: Average Return = -21.0\n",
      "step = 162200: loss = 0.0008508127648383379\n",
      "step = 162400: loss = 0.01819118857383728\n",
      "step = 162600: loss = 0.014898724853992462\n",
      "step = 162800: loss = 0.0016082744114100933\n",
      "step = 163000: loss = 0.0005003240657970309\n",
      "step = 163000: Average Return = -21.0\n",
      "step = 163200: loss = 0.0010342071764171124\n",
      "step = 163400: loss = 0.01910114474594593\n",
      "step = 163600: loss = 0.0010007091332226992\n",
      "step = 163800: loss = 0.0007561828242614865\n",
      "step = 164000: loss = 0.007778056897222996\n",
      "step = 164000: Average Return = -20.0\n",
      "step = 164200: loss = 0.001019094604998827\n",
      "step = 164400: loss = 0.008887727744877338\n",
      "step = 164600: loss = 0.00041938567301258445\n",
      "step = 164800: loss = 0.0007538265199400485\n",
      "step = 165000: loss = 0.001199305523186922\n",
      "step = 165000: Average Return = -20.0\n",
      "step = 165200: loss = 0.001211873721331358\n",
      "step = 165400: loss = 0.000636301701888442\n",
      "step = 165600: loss = 0.0004017550963908434\n",
      "step = 165800: loss = 0.0018027538899332285\n",
      "step = 166000: loss = 0.0014259950257837772\n",
      "step = 166000: Average Return = -20.0\n",
      "step = 166200: loss = 0.0008999953279271722\n",
      "step = 166400: loss = 0.00081965810386464\n",
      "step = 166600: loss = 0.0011812024749815464\n",
      "step = 166800: loss = 0.006882886867970228\n",
      "step = 167000: loss = 0.007477106060832739\n",
      "step = 167000: Average Return = -20.0\n",
      "step = 167200: loss = 0.0006236034678295255\n",
      "step = 167400: loss = 0.0008798431372269988\n",
      "step = 167600: loss = 0.0013738370034843683\n",
      "step = 167800: loss = 0.0009647333063185215\n",
      "step = 168000: loss = 0.005445957183837891\n",
      "step = 168000: Average Return = -20.0\n",
      "step = 168200: loss = 0.00249142455868423\n",
      "step = 168400: loss = 0.000652758521027863\n",
      "step = 168600: loss = 0.0013162796385586262\n",
      "step = 168800: loss = 0.00034261555992998183\n",
      "step = 169000: loss = 0.015558797866106033\n",
      "step = 169000: Average Return = -20.0\n",
      "step = 169200: loss = 0.004374953918159008\n",
      "step = 169400: loss = 0.0006479480070993304\n",
      "step = 169600: loss = 0.012699296697974205\n",
      "step = 169800: loss = 0.0009563464554958045\n",
      "step = 170000: loss = 0.0007241393905133009\n",
      "step = 170000: Average Return = -21.0\n",
      "step = 170200: loss = 0.0029371939599514008\n",
      "step = 170400: loss = 0.0003855298855341971\n",
      "step = 170600: loss = 0.0004935699980705976\n",
      "step = 170800: loss = 0.001438505481928587\n",
      "step = 171000: loss = 0.0005238854791969061\n",
      "step = 171000: Average Return = -20.0\n",
      "step = 171200: loss = 0.0007533103926107287\n",
      "step = 171400: loss = 0.0016931999707594514\n",
      "step = 171600: loss = 0.0026519151870161295\n",
      "step = 171800: loss = 0.0009789523901417851\n",
      "step = 172000: loss = 0.0012923700269311666\n",
      "step = 172000: Average Return = -21.0\n",
      "step = 172200: loss = 0.006115170195698738\n",
      "step = 172400: loss = 0.002077780431136489\n",
      "step = 172600: loss = 0.0007727732299827039\n",
      "step = 172800: loss = 0.0010268579935654998\n",
      "step = 173000: loss = 0.001659404719248414\n",
      "step = 173000: Average Return = -19.0\n",
      "step = 173200: loss = 0.0018053539097309113\n",
      "step = 173400: loss = 0.0012112663825973868\n",
      "step = 173600: loss = 0.001872616121545434\n",
      "step = 173800: loss = 0.0005106786848045886\n",
      "step = 174000: loss = 0.0005242876941338181\n",
      "step = 174000: Average Return = -20.0\n",
      "step = 174200: loss = 0.0007302936865016818\n",
      "step = 174400: loss = 0.0017207616474479437\n",
      "step = 174600: loss = 0.0003494322008918971\n",
      "step = 174800: loss = 0.0007739544962532818\n",
      "step = 175000: loss = 0.0030689463019371033\n",
      "step = 175000: Average Return = -16.0\n",
      "step = 175200: loss = 0.0005764102097600698\n",
      "step = 175400: loss = 0.00041141625843010843\n",
      "step = 175600: loss = 0.0007176710059866309\n",
      "step = 175800: loss = 0.00037637489731423557\n",
      "step = 176000: loss = 0.00036282779183238745\n",
      "step = 176000: Average Return = -21.0\n",
      "step = 176200: loss = 0.0008817640482448041\n",
      "step = 176400: loss = 0.0003701268578879535\n",
      "step = 176600: loss = 0.0005302231293171644\n",
      "step = 176800: loss = 0.0008238083682954311\n",
      "step = 177000: loss = 0.0014468625886365771\n",
      "step = 177000: Average Return = -21.0\n",
      "step = 177200: loss = 0.0007268995395861566\n",
      "step = 177400: loss = 0.0005150079960003495\n",
      "step = 177600: loss = 0.0036984796170145273\n",
      "step = 177800: loss = 0.0008803950040601194\n",
      "step = 178000: loss = 0.0009062608587555587\n",
      "step = 178000: Average Return = -21.0\n",
      "step = 178200: loss = 0.0004966449923813343\n",
      "step = 178400: loss = 0.0007224204018712044\n",
      "step = 178600: loss = 0.00791170634329319\n",
      "step = 178800: loss = 0.000749245926272124\n",
      "step = 179000: loss = 0.0010219760006293654\n",
      "step = 179000: Average Return = -21.0\n",
      "step = 179200: loss = 0.002753501757979393\n",
      "step = 179400: loss = 0.0010328971548005939\n",
      "step = 179600: loss = 0.0075486754067242146\n",
      "step = 179800: loss = 0.0011716679437085986\n",
      "step = 180000: loss = 0.0007413359126076102\n",
      "step = 180000: Average Return = -20.0\n",
      "step = 180200: loss = 0.002048913622274995\n",
      "step = 180400: loss = 0.0010366612114012241\n",
      "step = 180600: loss = 0.003927742596715689\n",
      "step = 180800: loss = 0.0010089732240885496\n",
      "step = 181000: loss = 0.000521287671290338\n",
      "step = 181000: Average Return = -20.0\n",
      "step = 181200: loss = 0.0007350983214564621\n",
      "step = 181400: loss = 0.00108707242179662\n",
      "step = 181600: loss = 0.0009783681016415358\n",
      "step = 181800: loss = 0.0010065939277410507\n",
      "step = 182000: loss = 0.0008423433755524457\n",
      "step = 182000: Average Return = -21.0\n",
      "step = 182200: loss = 0.0006134732393547893\n",
      "step = 182400: loss = 0.0015206963289529085\n",
      "step = 182600: loss = 0.0003639582428149879\n",
      "step = 182800: loss = 0.0006864453898742795\n",
      "step = 183000: loss = 0.002183274831622839\n",
      "step = 183000: Average Return = -20.0\n",
      "step = 183200: loss = 0.0005829590372741222\n",
      "step = 183400: loss = 0.000964915263466537\n",
      "step = 183600: loss = 0.003928007557988167\n",
      "step = 183800: loss = 0.0011315270094200969\n",
      "step = 184000: loss = 0.0011345286620780826\n",
      "step = 184000: Average Return = -21.0\n",
      "step = 184200: loss = 0.0006542641203850508\n",
      "step = 184400: loss = 0.0006471298402175307\n",
      "step = 184600: loss = 0.0006838897243142128\n",
      "step = 184800: loss = 0.000866966147441417\n",
      "step = 185000: loss = 0.0007309295469895005\n",
      "step = 185000: Average Return = -20.0\n",
      "step = 185200: loss = 0.004246526397764683\n",
      "step = 185400: loss = 0.001155089121311903\n",
      "step = 185600: loss = 0.014435339719057083\n",
      "step = 185800: loss = 0.00047659550909884274\n",
      "step = 186000: loss = 0.0009545419015921652\n",
      "step = 186000: Average Return = -21.0\n",
      "step = 186200: loss = 0.0009038099087774754\n",
      "step = 186400: loss = 0.0018730832962319255\n",
      "step = 186600: loss = 0.005661695264279842\n",
      "step = 186800: loss = 0.0008588274358771741\n",
      "step = 187000: loss = 0.0003671386803034693\n",
      "step = 187000: Average Return = -19.0\n",
      "step = 187200: loss = 0.0005571765941567719\n",
      "step = 187400: loss = 0.0004321293090470135\n",
      "step = 187600: loss = 0.00201215036213398\n",
      "step = 187800: loss = 0.0029668081551790237\n",
      "step = 188000: loss = 0.0013440197799354792\n",
      "step = 188000: Average Return = -19.0\n",
      "step = 188200: loss = 0.0018131296383216977\n",
      "step = 188400: loss = 0.0002800706715788692\n",
      "step = 188600: loss = 0.0009543774649500847\n",
      "step = 188800: loss = 0.0020654445979744196\n",
      "step = 189000: loss = 0.011921151541173458\n",
      "step = 189000: Average Return = -21.0\n",
      "step = 189200: loss = 0.001406294759362936\n",
      "step = 189400: loss = 0.001272624242119491\n",
      "step = 189600: loss = 0.0014921652618795633\n",
      "step = 189800: loss = 0.003115388099104166\n",
      "step = 190000: loss = 0.0011626158375293016\n",
      "step = 190000: Average Return = -20.0\n",
      "step = 190200: loss = 0.0005170819931663573\n",
      "step = 190400: loss = 0.016752012073993683\n",
      "step = 190600: loss = 0.003315551672130823\n",
      "step = 190800: loss = 0.0015976059949025512\n",
      "step = 191000: loss = 0.0098926005885005\n",
      "step = 191000: Average Return = -21.0\n",
      "step = 191200: loss = 0.0002300934720551595\n",
      "step = 191400: loss = 0.0003972025588154793\n",
      "step = 191600: loss = 0.0010973336175084114\n",
      "step = 191800: loss = 0.0008077671518549323\n",
      "step = 192000: loss = 0.0004959013313055038\n",
      "step = 192000: Average Return = -21.0\n",
      "step = 192200: loss = 0.0011095314985141158\n",
      "step = 192400: loss = 0.0005930701154284179\n",
      "step = 192600: loss = 0.0004215360968373716\n",
      "step = 192800: loss = 0.000672836322337389\n",
      "step = 193000: loss = 0.00033602965413592756\n",
      "step = 193000: Average Return = -20.0\n",
      "step = 193200: loss = 0.011965484358370304\n",
      "step = 193400: loss = 0.001531946356408298\n",
      "step = 193600: loss = 0.00167548144236207\n",
      "step = 193800: loss = 0.001771901035681367\n",
      "step = 194000: loss = 0.000444377918029204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 194000: Average Return = -21.0\n",
      "step = 194200: loss = 0.0026673292741179466\n",
      "step = 194400: loss = 0.0006898726569488645\n",
      "step = 194600: loss = 0.001598409260623157\n",
      "step = 194800: loss = 0.0002579345309641212\n",
      "step = 195000: loss = 0.007028692867606878\n",
      "step = 195000: Average Return = -21.0\n",
      "step = 195200: loss = 0.001135858241468668\n",
      "step = 195400: loss = 0.002130062086507678\n",
      "step = 195600: loss = 0.0008886315627023578\n",
      "step = 195800: loss = 0.0007873630383983254\n",
      "step = 196000: loss = 0.0004377133445814252\n",
      "step = 196000: Average Return = -21.0\n",
      "step = 196200: loss = 0.0003313653578516096\n",
      "step = 196400: loss = 0.0029368342366069555\n",
      "step = 196600: loss = 0.0003199053753633052\n",
      "step = 196800: loss = 0.0004893522709608078\n",
      "step = 197000: loss = 0.00070315005723387\n",
      "step = 197000: Average Return = -21.0\n",
      "step = 197200: loss = 0.003513248870149255\n",
      "step = 197400: loss = 0.0011069028405472636\n",
      "step = 197600: loss = 0.0007173326448537409\n",
      "step = 197800: loss = 0.0009881636360660195\n",
      "step = 198000: loss = 0.0011399349896237254\n",
      "step = 198000: Average Return = -21.0\n",
      "step = 198200: loss = 0.0002986124891322106\n",
      "step = 198400: loss = 0.0010599121451377869\n",
      "step = 198600: loss = 0.0007647875463590026\n",
      "step = 198800: loss = 0.0009218387422151864\n",
      "step = 199000: loss = 0.0037749181501567364\n",
      "step = 199000: Average Return = -20.0\n",
      "step = 199200: loss = 0.0008560165297240019\n",
      "step = 199400: loss = 0.0004814834101125598\n",
      "step = 199600: loss = 0.001482822117395699\n",
      "step = 199800: loss = 0.0044434554874897\n",
      "step = 200000: loss = 0.0006184711237438023\n",
      "step = 200000: Average Return = -21.0\n",
      "step = 200200: loss = 0.0012500195298343897\n",
      "step = 200400: loss = 0.00042925067828036845\n",
      "step = 200600: loss = 0.0007379812304861844\n",
      "step = 200800: loss = 0.004155581817030907\n",
      "step = 201000: loss = 0.0035181408748030663\n",
      "step = 201000: Average Return = -20.0\n",
      "step = 201200: loss = 0.003140420187264681\n",
      "step = 201400: loss = 0.015379184857010841\n",
      "step = 201600: loss = 0.005396345164626837\n",
      "step = 201800: loss = 0.0005503571592271328\n",
      "step = 202000: loss = 0.000506376032717526\n",
      "step = 202000: Average Return = -20.0\n",
      "step = 202200: loss = 0.0017270823009312153\n",
      "step = 202400: loss = 0.0010058514308184385\n",
      "step = 202600: loss = 0.0004458713810890913\n",
      "step = 202800: loss = 0.0016950282733887434\n",
      "step = 203000: loss = 0.0007648892933502793\n",
      "step = 203000: Average Return = -21.0\n",
      "step = 203200: loss = 0.00023560586851090193\n",
      "step = 203400: loss = 0.0016273625660687685\n",
      "step = 203600: loss = 0.0005047495360486209\n",
      "step = 203800: loss = 0.0003406817850191146\n",
      "step = 204000: loss = 0.0013424891512840986\n",
      "step = 204000: Average Return = -21.0\n",
      "step = 204200: loss = 0.0014852059539407492\n",
      "step = 204400: loss = 0.006052379030734301\n",
      "step = 204600: loss = 0.0015754962805658579\n",
      "step = 204800: loss = 0.014269514009356499\n",
      "step = 205000: loss = 0.004992793779820204\n",
      "step = 205000: Average Return = -20.0\n",
      "step = 205200: loss = 0.0011047087609767914\n",
      "step = 205400: loss = 0.004724570084363222\n",
      "step = 205600: loss = 0.0005580627475865185\n",
      "step = 205800: loss = 0.005915675777941942\n",
      "step = 206000: loss = 0.002258160151541233\n",
      "step = 206000: Average Return = -21.0\n",
      "step = 206200: loss = 0.0007565062260255218\n",
      "step = 206400: loss = 0.000436574628110975\n",
      "step = 206600: loss = 0.00041433289879933\n",
      "step = 206800: loss = 0.001996326958760619\n",
      "step = 207000: loss = 0.001620651688426733\n",
      "step = 207000: Average Return = -21.0\n",
      "step = 207200: loss = 0.0006757626542821527\n",
      "step = 207400: loss = 0.002524227136746049\n",
      "step = 207600: loss = 0.0006069101509638131\n",
      "step = 207800: loss = 0.0015825619921088219\n",
      "step = 208000: loss = 0.0060083502903580666\n",
      "step = 208000: Average Return = -21.0\n",
      "step = 208200: loss = 0.0006399154081009328\n",
      "step = 208400: loss = 0.0054443902336061\n",
      "step = 208600: loss = 0.0012943644542247057\n",
      "step = 208800: loss = 0.000615661614574492\n",
      "step = 209000: loss = 0.0007750684744678438\n",
      "step = 209000: Average Return = -21.0\n",
      "step = 209200: loss = 0.0006642742082476616\n",
      "step = 209400: loss = 0.007965856231749058\n",
      "step = 209600: loss = 0.0010532124433666468\n",
      "step = 209800: loss = 0.0010182815603911877\n",
      "step = 210000: loss = 0.0009720023954287171\n",
      "step = 210000: Average Return = -21.0\n",
      "step = 210200: loss = 0.0005614631809294224\n",
      "step = 210400: loss = 0.0011409504804760218\n",
      "step = 210600: loss = 0.0005275997100397944\n",
      "step = 210800: loss = 0.0004235213273204863\n",
      "step = 211000: loss = 0.0006068053771741688\n",
      "step = 211000: Average Return = -21.0\n",
      "step = 211200: loss = 0.0009183376096189022\n",
      "step = 211400: loss = 0.0007820527534931898\n",
      "step = 211600: loss = 0.0014309174148365855\n",
      "step = 211800: loss = 0.0008516981033608317\n",
      "step = 212000: loss = 0.0003795217489823699\n",
      "step = 212000: Average Return = -21.0\n",
      "step = 212200: loss = 0.0005542901926673949\n",
      "step = 212400: loss = 0.0025441658217459917\n",
      "step = 212600: loss = 0.0006263592513278127\n",
      "step = 212800: loss = 0.0003978617605753243\n",
      "step = 213000: loss = 0.0016598115907981992\n",
      "step = 213000: Average Return = -21.0\n",
      "step = 213200: loss = 0.0026438720524311066\n",
      "step = 213400: loss = 0.002604620298370719\n",
      "step = 213600: loss = 0.006296115927398205\n",
      "step = 213800: loss = 0.001125564333051443\n",
      "step = 214000: loss = 0.0003649178543128073\n",
      "step = 214000: Average Return = -21.0\n",
      "step = 214200: loss = 0.0005056362715549767\n",
      "step = 214400: loss = 0.0004610615433193743\n",
      "step = 214600: loss = 0.0004664833249989897\n",
      "step = 214800: loss = 0.016648951917886734\n",
      "step = 215000: loss = 0.0005181796150282025\n",
      "step = 215000: Average Return = -21.0\n",
      "step = 215200: loss = 0.0018642741488292813\n",
      "step = 215400: loss = 0.000754470587708056\n",
      "step = 215600: loss = 0.0006367879686877131\n",
      "step = 215800: loss = 0.001432708348147571\n",
      "step = 216000: loss = 0.0009013473754748702\n",
      "step = 216000: Average Return = -21.0\n",
      "step = 216200: loss = 0.0010409316746518016\n",
      "step = 216400: loss = 0.000537131680175662\n",
      "step = 216600: loss = 0.010654084384441376\n",
      "step = 216800: loss = 0.005988797172904015\n",
      "step = 217000: loss = 0.0027742553502321243\n",
      "step = 217000: Average Return = -21.0\n",
      "step = 217200: loss = 0.0013221793342381716\n",
      "step = 217400: loss = 0.0011576395481824875\n",
      "step = 217600: loss = 0.0004699119890574366\n",
      "step = 217800: loss = 0.0015549538657069206\n",
      "step = 218000: loss = 0.00043322646524757147\n",
      "step = 218000: Average Return = -20.0\n",
      "step = 218200: loss = 0.0002970499626826495\n",
      "step = 218400: loss = 0.00030095214606262743\n",
      "step = 218600: loss = 0.0012399955885484815\n",
      "step = 218800: loss = 0.002934496384114027\n",
      "step = 219000: loss = 0.00029247498605400324\n",
      "step = 219000: Average Return = -21.0\n",
      "step = 219200: loss = 0.001681264489889145\n",
      "step = 219400: loss = 0.0005738746840506792\n",
      "step = 219600: loss = 0.001226719468832016\n",
      "step = 219800: loss = 0.01690923422574997\n",
      "step = 220000: loss = 0.001832652953453362\n",
      "step = 220000: Average Return = -21.0\n",
      "step = 220200: loss = 0.0006694886833429337\n",
      "step = 220400: loss = 0.018128858879208565\n",
      "step = 220600: loss = 0.015085838735103607\n",
      "step = 220800: loss = 0.005026150029152632\n",
      "step = 221000: loss = 0.0061738318763673306\n",
      "step = 221000: Average Return = -21.0\n",
      "step = 221200: loss = 0.0023562742862850428\n",
      "step = 221400: loss = 0.0009879895951598883\n",
      "step = 221600: loss = 0.001191197196021676\n",
      "step = 221800: loss = 0.01575387641787529\n",
      "step = 222000: loss = 0.0009422568837180734\n",
      "step = 222000: Average Return = -21.0\n",
      "step = 222200: loss = 0.00326943164691329\n",
      "step = 222400: loss = 0.0009769564494490623\n",
      "step = 222600: loss = 0.0007110883016139269\n",
      "step = 222800: loss = 0.011407756246626377\n",
      "step = 223000: loss = 0.00024277280317619443\n",
      "step = 223000: Average Return = -20.0\n",
      "step = 223200: loss = 0.006003405433148146\n",
      "step = 223400: loss = 0.001416709739714861\n",
      "step = 223600: loss = 0.0006353050703182817\n",
      "step = 223800: loss = 0.0017904984997585416\n",
      "step = 224000: loss = 0.0006955601857043803\n",
      "step = 224000: Average Return = -20.0\n",
      "step = 224200: loss = 0.007850809954106808\n",
      "step = 224400: loss = 0.0007379376329481602\n",
      "step = 224600: loss = 0.001233382849022746\n",
      "step = 224800: loss = 0.0005243616760708392\n",
      "step = 225000: loss = 0.0014153795782476664\n",
      "step = 225000: Average Return = -21.0\n",
      "step = 225200: loss = 0.004706131760030985\n",
      "step = 225400: loss = 0.0057479385286569595\n",
      "step = 225600: loss = 0.0006654865574091673\n",
      "step = 225800: loss = 0.0008716937736608088\n",
      "step = 226000: loss = 0.0006534791318699718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 226000: Average Return = -20.0\n",
      "step = 226200: loss = 0.00023639613937120885\n",
      "step = 226400: loss = 0.0038102674297988415\n",
      "step = 226600: loss = 0.003603864461183548\n",
      "step = 226800: loss = 0.009735033847391605\n",
      "step = 227000: loss = 0.0007818237645551562\n",
      "step = 227000: Average Return = -21.0\n",
      "step = 227200: loss = 0.0013819357845932245\n",
      "step = 227400: loss = 0.0011040463577955961\n",
      "step = 227600: loss = 0.0007436675950884819\n",
      "step = 227800: loss = 0.0011405372060835361\n",
      "step = 228000: loss = 0.0028640739619731903\n",
      "step = 228000: Average Return = -20.0\n",
      "step = 228200: loss = 0.0003617395996116102\n",
      "step = 228400: loss = 0.0017869769362732768\n",
      "step = 228600: loss = 0.0011889832094311714\n",
      "step = 228800: loss = 0.005969197954982519\n",
      "step = 229000: loss = 0.0011662275064736605\n",
      "step = 229000: Average Return = -21.0\n",
      "step = 229200: loss = 0.0004707594634965062\n",
      "step = 229400: loss = 0.0016497515607625246\n",
      "step = 229600: loss = 0.0005916774389334023\n",
      "step = 229800: loss = 0.0010848059318959713\n",
      "step = 230000: loss = 0.002526313066482544\n",
      "step = 230000: Average Return = -19.0\n",
      "step = 230200: loss = 0.0008064898429438472\n",
      "step = 230400: loss = 0.0002936274977400899\n",
      "step = 230600: loss = 0.0013239604886621237\n",
      "step = 230800: loss = 0.0007929194835014641\n",
      "step = 231000: loss = 0.001126859337091446\n",
      "step = 231000: Average Return = -21.0\n",
      "step = 231200: loss = 0.0005344517412595451\n",
      "step = 231400: loss = 0.0017062918050214648\n",
      "step = 231600: loss = 0.0008106155437417328\n",
      "step = 231800: loss = 0.0005581618752330542\n",
      "step = 232000: loss = 0.0013184037525206804\n",
      "step = 232000: Average Return = -21.0\n",
      "step = 232200: loss = 0.014133043587207794\n",
      "step = 232400: loss = 0.025831041857600212\n",
      "step = 232600: loss = 0.0006540400208905339\n",
      "step = 232800: loss = 0.0004668750916607678\n",
      "step = 233000: loss = 0.009531808085739613\n",
      "step = 233000: Average Return = -21.0\n",
      "step = 233200: loss = 0.001073711784556508\n",
      "step = 233400: loss = 0.013172119855880737\n",
      "step = 233600: loss = 0.0006606002571061254\n",
      "step = 233800: loss = 0.0019441416952759027\n",
      "step = 234000: loss = 0.001311657833866775\n",
      "step = 234000: Average Return = -21.0\n",
      "step = 234200: loss = 0.0009511736570857465\n",
      "step = 234400: loss = 0.0004017501778434962\n",
      "step = 234600: loss = 0.012305880896747112\n",
      "step = 234800: loss = 0.001096926280297339\n",
      "step = 235000: loss = 0.0008055617799982429\n",
      "step = 235000: Average Return = -20.0\n",
      "step = 235200: loss = 0.00251651625148952\n",
      "step = 235400: loss = 0.0006664489628747106\n",
      "step = 235600: loss = 0.0007102381787262857\n",
      "step = 235800: loss = 0.0011695390567183495\n",
      "step = 236000: loss = 0.0030879273544996977\n",
      "step = 236000: Average Return = -21.0\n",
      "step = 236200: loss = 0.004758007358759642\n",
      "step = 236400: loss = 0.0010719578713178635\n",
      "step = 236600: loss = 0.0009350554901175201\n",
      "step = 236800: loss = 0.0009139890316873789\n",
      "step = 237000: loss = 0.0004316982813179493\n",
      "step = 237000: Average Return = -21.0\n",
      "step = 237200: loss = 0.0006663348176516593\n",
      "step = 237400: loss = 0.0006114833522588015\n",
      "step = 237600: loss = 0.0030651455745100975\n",
      "step = 237800: loss = 0.0013892408460378647\n",
      "step = 238000: loss = 0.0007631364860571921\n",
      "step = 238000: Average Return = -19.0\n",
      "step = 238200: loss = 0.0007878659525886178\n",
      "step = 238400: loss = 0.01685403473675251\n",
      "step = 238600: loss = 0.0008453281479887664\n",
      "step = 238800: loss = 0.0008547418983653188\n",
      "step = 239000: loss = 0.0008694992284290493\n",
      "step = 239000: Average Return = -21.0\n",
      "step = 239200: loss = 0.002525284420698881\n",
      "step = 239400: loss = 0.0008870786987245083\n",
      "step = 239600: loss = 0.0004877890460193157\n",
      "step = 239800: loss = 0.007541821338236332\n",
      "step = 240000: loss = 0.0005872363690286875\n",
      "step = 240000: Average Return = -20.0\n",
      "step = 240200: loss = 0.0006591955898329616\n",
      "step = 240400: loss = 0.0006125427316874266\n",
      "step = 240600: loss = 0.005211094859987497\n",
      "step = 240800: loss = 0.0004114907933399081\n",
      "step = 241000: loss = 0.0013384646736085415\n",
      "step = 241000: Average Return = -20.0\n",
      "step = 241200: loss = 0.0093466155230999\n",
      "step = 241400: loss = 0.0011956407688558102\n",
      "step = 241600: loss = 0.0006394751253537834\n",
      "step = 241800: loss = 0.0019537792541086674\n",
      "step = 242000: loss = 0.04182692617177963\n",
      "step = 242000: Average Return = -20.0\n",
      "step = 242200: loss = 0.0360250249505043\n",
      "step = 242400: loss = 0.0005838745273649693\n",
      "step = 242600: loss = 0.0006185525562614202\n",
      "step = 242800: loss = 0.0005577042466029525\n",
      "step = 243000: loss = 0.01556633971631527\n",
      "step = 243000: Average Return = -20.0\n",
      "step = 243200: loss = 0.000756465713493526\n",
      "step = 243400: loss = 0.0013464560033753514\n",
      "step = 243600: loss = 0.0017745046643540263\n",
      "step = 243800: loss = 0.0018829067703336477\n",
      "step = 244000: loss = 0.0015012279618531466\n",
      "step = 244000: Average Return = -20.0\n",
      "step = 244200: loss = 0.006428977940231562\n",
      "step = 244400: loss = 0.0013443123316392303\n",
      "step = 244600: loss = 0.0005874198395758867\n",
      "step = 244800: loss = 0.0026225775945931673\n",
      "step = 245000: loss = 0.007183711510151625\n",
      "step = 245000: Average Return = -20.0\n",
      "step = 245200: loss = 0.0017595455283299088\n",
      "step = 245400: loss = 0.0009032784146256745\n",
      "step = 245600: loss = 0.0017058977391570807\n",
      "step = 245800: loss = 0.013038444332778454\n",
      "step = 246000: loss = 0.001961131114512682\n",
      "step = 246000: Average Return = -21.0\n",
      "step = 246200: loss = 0.0011085669975727797\n",
      "step = 246400: loss = 0.0008595993858762085\n",
      "step = 246600: loss = 0.0024199127219617367\n",
      "step = 246800: loss = 0.004838122520595789\n",
      "step = 247000: loss = 0.0009023986640386283\n",
      "step = 247000: Average Return = -19.0\n",
      "step = 247200: loss = 0.0034678413067013025\n",
      "step = 247400: loss = 0.00042946598841808736\n",
      "step = 247600: loss = 0.002817166270688176\n",
      "step = 247800: loss = 0.0009874652605503798\n",
      "step = 248000: loss = 0.0026104638818651438\n",
      "step = 248000: Average Return = -20.0\n",
      "step = 248200: loss = 0.002454793080687523\n",
      "step = 248400: loss = 0.002808673307299614\n",
      "step = 248600: loss = 0.001045391196385026\n",
      "step = 248800: loss = 0.011268222704529762\n",
      "step = 249000: loss = 0.0006351604824885726\n",
      "step = 249000: Average Return = -21.0\n",
      "step = 249200: loss = 0.00950370542705059\n",
      "step = 249400: loss = 0.004745058715343475\n",
      "step = 249600: loss = 0.0009398168185725808\n",
      "step = 249800: loss = 0.005622311029583216\n",
      "step = 250000: loss = 0.0023291967809200287\n",
      "step = 250000: Average Return = -21.0\n",
      "step = 250200: loss = 0.0009173353901132941\n",
      "step = 250400: loss = 0.0006785531877540052\n",
      "step = 250600: loss = 0.0005867633153684437\n",
      "step = 250800: loss = 0.0014765484957024455\n",
      "step = 251000: loss = 0.0010717929108068347\n",
      "step = 251000: Average Return = -20.0\n",
      "step = 251200: loss = 0.0006297373911365867\n",
      "step = 251400: loss = 0.008739039301872253\n",
      "step = 251600: loss = 0.0010147001594305038\n",
      "step = 251800: loss = 0.0018108253134414554\n",
      "step = 252000: loss = 0.0005926273297518492\n",
      "step = 252000: Average Return = -20.0\n",
      "step = 252200: loss = 0.0016903840005397797\n",
      "step = 252400: loss = 0.0009381115669384599\n",
      "step = 252600: loss = 0.0014249916421249509\n",
      "step = 252800: loss = 0.0010242692660540342\n",
      "step = 253000: loss = 0.006602148059755564\n",
      "step = 253000: Average Return = -21.0\n",
      "step = 253200: loss = 0.001670934958383441\n",
      "step = 253400: loss = 0.0019404652994126081\n",
      "step = 253600: loss = 0.002390955574810505\n",
      "step = 253800: loss = 0.0033801905810832977\n",
      "step = 254000: loss = 0.009894023649394512\n",
      "step = 254000: Average Return = -21.0\n",
      "step = 254200: loss = 0.0005058504757471383\n",
      "step = 254400: loss = 0.00039830058813095093\n",
      "step = 254600: loss = 0.015638722106814384\n",
      "step = 254800: loss = 0.0007017351454123855\n",
      "step = 255000: loss = 0.0005213869735598564\n",
      "step = 255000: Average Return = -20.0\n",
      "step = 255200: loss = 0.0008308362448588014\n",
      "step = 255400: loss = 0.0018532967660576105\n",
      "step = 255600: loss = 0.0007001680787652731\n",
      "step = 255800: loss = 0.0011693374253809452\n",
      "step = 256000: loss = 0.003905537538230419\n",
      "step = 256000: Average Return = -21.0\n",
      "step = 256200: loss = 0.003373861312866211\n",
      "step = 256400: loss = 0.006357003003358841\n",
      "step = 256600: loss = 0.003041396150365472\n",
      "step = 256800: loss = 0.0009485327173024416\n",
      "step = 257000: loss = 0.02011813595890999\n",
      "step = 257000: Average Return = -21.0\n",
      "step = 257200: loss = 0.003807533998042345\n",
      "step = 257400: loss = 0.0018606773810461164\n",
      "step = 257600: loss = 0.0014183784369379282\n",
      "step = 257800: loss = 0.0011048985179513693\n",
      "step = 258000: loss = 0.0017230876255780458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 258000: Average Return = -20.0\n",
      "step = 258200: loss = 0.0011486544972285628\n",
      "step = 258400: loss = 0.014058304950594902\n",
      "step = 258600: loss = 0.012851867824792862\n",
      "step = 258800: loss = 0.0010011850390583277\n",
      "step = 259000: loss = 0.020419809967279434\n",
      "step = 259000: Average Return = -21.0\n",
      "step = 259200: loss = 0.0010211452608928084\n",
      "step = 259400: loss = 0.0006579284672625363\n",
      "step = 259600: loss = 0.0008935339283198118\n",
      "step = 259800: loss = 0.0012441595317795873\n",
      "step = 260000: loss = 0.0033949839416891336\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-106bfe411d92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mavg_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_avg_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_eval_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'step = {0}: Average Return = {1}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_return\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mreturns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_return\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-dc8ee8d7f3c1>\u001b[0m in \u001b[0;36mcompute_avg_return\u001b[0;34m(environment, policy, num_episodes)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_last\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m       \u001b[0maction_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m       \u001b[0mtime_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0mepisode_return\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RL/lib/python3.7/site-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36maction\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_automatic_state_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m       \u001b[0mpolicy_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_reset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclip_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RL/lib/python3.7/site-packages/tf_agents/utils/common.py\u001b[0m in \u001b[0;36mwith_check_resource_vars\u001b[0;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;31m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;31m# autodep-like behavior is already expected of fn.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresource_variables_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMISSING_RESOURCE_VARIABLES_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RL/lib/python3.7/site-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36m_action\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    467\u001b[0m     \"\"\"\n\u001b[1;32m    468\u001b[0m     \u001b[0mseed_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeedStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msalt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ppo_policy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m     \u001b[0mdistribution_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m     actions = tf.nest.map_structure(\n\u001b[1;32m    471\u001b[0m         \u001b[0;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreparameterized_sampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RL/lib/python3.7/site-packages/tf_agents/policies/greedy_policy.py\u001b[0m in \u001b[0;36m_distribution\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     distribution_step = self._wrapped_policy.distribution(\n\u001b[0;32m---> 77\u001b[0;31m         time_step, policy_state)\n\u001b[0m\u001b[1;32m     78\u001b[0m     return policy_step.PolicyStep(\n\u001b[1;32m     79\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistribution_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RL/lib/python3.7/site-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36mdistribution\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_automatic_state_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m       \u001b[0mpolicy_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_reset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memit_log_probability\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m       \u001b[0;31m# This here is set only for compatibility with info_spec in constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RL/lib/python3.7/site-packages/tf_agents/policies/q_policy.py\u001b[0m in \u001b[0;36m_distribution\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     q_values, policy_state = self._q_network(\n\u001b[0;32m--> 123\u001b[0;31m         network_observation, time_step.step_type, policy_state)\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;31m# TODO(b/122314058): Validate and enforce that sampling distributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RL/lib/python3.7/site-packages/tf_agents/networks/network.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnetwork_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m       \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    821\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RL/lib/python3.7/site-packages/tf_agents/networks/q_network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, observation, step_type, network_state, training)\u001b[0m\n\u001b[1;32m    145\u001b[0m     state, network_state = self._encoder(\n\u001b[1;32m    146\u001b[0m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnetwork_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         training=training)\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0mq_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_q_value_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mq_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RL/lib/python3.7/site-packages/tf_agents/networks/network.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnetwork_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m       \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    821\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RL/lib/python3.7/site-packages/tf_agents/networks/encoding_network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_postprocessing_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m       \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_squash\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    821\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    218\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'NCHW'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'NHWC'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mbias_add\u001b[0;34m(value, bias, data_format, name)\u001b[0m\n\u001b[1;32m   2756\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2757\u001b[0m       return gen_nn_ops.bias_add(\n\u001b[0;32m-> 2758\u001b[0;31m           value, bias, data_format=data_format, name=name)\n\u001b[0m\u001b[1;32m   2759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mbias_add\u001b[0;34m(value, bias, data_format, name)\u001b[0m\n\u001b[1;32m    670\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[1;32m    671\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"BiasAdd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m         tld.op_callbacks, value, bias, \"data_format\", data_format)\n\u001b[0m\u001b[1;32m    673\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "  %%time\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  for _ in range(collect_steps_per_iteration):\n",
    "    collect_step(train_env, agent.collect_policy, replay_buffer)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience).loss\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "    returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## iterations = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')\n",
    "plt.title('Pong Training')\n",
    "plt.ylim(top=-15)\n",
    "plt.savefig('Pong_Training_Reward_Plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_mp4(filename):\n",
    "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
    "  video = open(filename,'rb').read()\n",
    "  b64 = base64.b64encode(video)\n",
    "  tag = '''\n",
    "  <video width=\"640\" height=\"480\" controls>\n",
    "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "  </video>'''.format(b64.decode())\n",
    "\n",
    "  return IPython.display.HTML(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (210, 160) to (224, 160) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to None (risking incompatibility). You may also see a FFMPEG warning concerning speedloss due to data not being aligned.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-7853964a9865>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mcreate_policy_eval_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"trained-agent\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-7853964a9865>\u001b[0m in \u001b[0;36mcreate_policy_eval_video\u001b[0;34m(policy, filename, num_episodes, fps)\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_py_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_last\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0maction_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mtime_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_py_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RL/lib/python3.7/site-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36maction\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_automatic_state_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m       \u001b[0mpolicy_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_reset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclip_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RL/lib/python3.7/site-packages/tf_agents/utils/common.py\u001b[0m in \u001b[0;36mwith_check_resource_vars\u001b[0;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;31m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;31m# autodep-like behavior is already expected of fn.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresource_variables_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMISSING_RESOURCE_VARIABLES_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RL/lib/python3.7/site-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36m_action\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    467\u001b[0m     \"\"\"\n\u001b[1;32m    468\u001b[0m     \u001b[0mseed_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeedStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msalt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ppo_policy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m     \u001b[0mdistribution_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m     actions = tf.nest.map_structure(\n\u001b[1;32m    471\u001b[0m         \u001b[0;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreparameterized_sampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RL/lib/python3.7/site-packages/tf_agents/policies/greedy_policy.py\u001b[0m in \u001b[0;36m_distribution\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     distribution_step = self._wrapped_policy.distribution(\n\u001b[0;32m---> 77\u001b[0;31m         time_step, policy_state)\n\u001b[0m\u001b[1;32m     78\u001b[0m     return policy_step.PolicyStep(\n\u001b[1;32m     79\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistribution_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RL/lib/python3.7/site-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36mdistribution\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_automatic_state_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m       \u001b[0mpolicy_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_reset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memit_log_probability\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m       \u001b[0;31m# This here is set only for compatibility with info_spec in constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RL/lib/python3.7/site-packages/tf_agents/policies/q_policy.py\u001b[0m in \u001b[0;36m_distribution\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     q_values, policy_state = self._q_network(\n\u001b[0;32m--> 123\u001b[0;31m         network_observation, time_step.step_type, policy_state)\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;31m# TODO(b/122314058): Validate and enforce that sampling distributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RL/lib/python3.7/site-packages/tf_agents/networks/network.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnetwork_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m       \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    821\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RL/lib/python3.7/site-packages/tf_agents/networks/q_network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, observation, step_type, network_state, training)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnetwork_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         training=training)\n\u001b[0;32m--> 148\u001b[0;31m     \u001b[0mq_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_q_value_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mq_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    821\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1142\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1144\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mbias_add\u001b[0;34m(value, bias, data_format, name)\u001b[0m\n\u001b[1;32m   2756\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2757\u001b[0m       return gen_nn_ops.bias_add(\n\u001b[0;32m-> 2758\u001b[0;31m           value, bias, data_format=data_format, name=name)\n\u001b[0m\u001b[1;32m   2759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mbias_add\u001b[0;34m(value, bias, data_format, name)\u001b[0m\n\u001b[1;32m    670\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[1;32m    671\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"BiasAdd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m         tld.op_callbacks, value, bias, \"data_format\", data_format)\n\u001b[0m\u001b[1;32m    673\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def create_policy_eval_video(policy, filename, num_episodes=5, fps=30):\n",
    "  filename = filename + \".mp4\"\n",
    "  with imageio.get_writer(filename, fps=fps) as video:\n",
    "    for _ in range(num_episodes):\n",
    "      time_step = eval_env.reset()\n",
    "      video.append_data(eval_py_env.render())\n",
    "      while not time_step.is_last():\n",
    "        action_step = policy.action(time_step)\n",
    "        time_step = eval_env.step(action_step.action)\n",
    "        video.append_data(eval_py_env.render())\n",
    "  return embed_mp4(filename)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "create_policy_eval_video(agent.policy, \"trained-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
