{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "from torchvision import transforms, datasets\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "if torch.cuda.is_available():\r\n",
    "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \r\n",
    "    print(\"Running on the GPU\")\r\n",
    "else:\r\n",
    "    device = torch.device(\"cpu\")\r\n",
    "    print(\"Running on the CPU\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "trainset = datasets.MNIST('',download=True,train=True,transform=transforms.Compose([transforms.ToTensor(),\r\n",
    "                                                                                  transforms.Normalize((0.5),(0.5))]))\r\n",
    "train_loader = DataLoader(trainset,batch_size=50,shuffle=True)\r\n",
    "testset = datasets.MNIST('',download=True,train=False,transform=transforms.Compose([transforms.ToTensor(),\r\n",
    "                                                                                  transforms.Normalize((0.5),(0.5))]))\r\n",
    "test_loader = DataLoader(testset,batch_size=50,shuffle=False)\r\n",
    "train_mnist,val_mnist = train_test_split(trainset.data,test_size=0.2,random_state=42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Function for Processing Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def make_mnist_domains(x,n_domains=4,n_missing_samples=100,missing_idxs=None):\r\n",
    "    \r\n",
    "    splits = np.split(torch.flatten(x,1,2),n_domains)\r\n",
    "    masks = [torch.ones_like(split) for split in splits]\r\n",
    "    \r\n",
    "    if missing_idxs == None: \r\n",
    "        missing_idxs = [np.random.choice(split.shape[1],n_missing_samples) for split in splits]\r\n",
    "\r\n",
    "    \r\n",
    "    for idxs,split,mask in zip(missing_idxs,splits,masks):\r\n",
    "        split[:,idxs] = 0\r\n",
    "        mask[:,idxs] = 0\r\n",
    "    \r\n",
    "    return splits, masks, missing_idxs\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get Splits and Masks"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "train_splits,train_masks,idxs = make_mnist_domains(train_mnist)\r\n",
    "val_splits,val_masks,idxs = make_mnist_domains(val_mnist,missing_idxs=idxs)\r\n",
    "test_splits,test_masks,idxs = make_mnist_domains(testset.data,missing_idxs=idxs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "train_splits[0].shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([12000, 784])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define the Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class domain_style_encoders(nn.Module):\r\n",
    "    def __init__(self,input_shape,n_domains,batch_size,device='cpu'):\r\n",
    "        super(domain_style_encoders,self).__init__()\r\n",
    "        \r\n",
    "        self.input_shape = input_shape\r\n",
    "        self.n_domains = n_domains\r\n",
    "        self.batch_size = batch_size\r\n",
    "        self.Encoder_List = nn.ModuleList().to(device)\r\n",
    "        self.device = device\r\n",
    "        \r\n",
    "        for i in range(n_domains):\r\n",
    "            self.Encoder_List.append(\r\n",
    "                nn.Sequential(\r\n",
    "                    nn.Linear(input_shape,input_shape),\r\n",
    "                    nn.ELU(),\r\n",
    "                    nn.Linear(input_shape,input_shape),\r\n",
    "                    nn.ELU()             \r\n",
    "                ).to(device)           \r\n",
    "            )\r\n",
    "            \r\n",
    "    def forward(self,x):\r\n",
    "        \r\n",
    "        x_dse_full = torch.Tensor(self.n_domains*self.batch_size,self.input_shape).to(self.device)\r\n",
    "        \r\n",
    "        for i in range(self.n_domains):\r\n",
    "            idxs = (i*self.batch_size,(i+1)*self.batch_size)\r\n",
    "            x_dse = x[idxs[0]:idxs[1]] + self.Encoder_List[i].forward(x[idxs[0]:idxs[1]])\r\n",
    "            x_dse_full[idxs[0]:idxs[1]] = x_dse"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class domain_style_decoders(nn.Module):\r\n",
    "    def __init__(self,input_shape,n_domains,batch_size,device='cpu'):\r\n",
    "        super(domain_style_encoders,self).__init__()\r\n",
    "        \r\n",
    "        self.input_shape = input_shape\r\n",
    "        self.n_domains = n_domains\r\n",
    "        self.batch_size = batch_size\r\n",
    "        self.Decoder_List = nn.ModuleList().to(device)\r\n",
    "        self.device = device\r\n",
    "        \r\n",
    "        for i in range(n_domains):\r\n",
    "            self.Decoder_List.append(\r\n",
    "                nn.Sequential(\r\n",
    "                    nn.Linear(input_shape,input_shape),\r\n",
    "                    nn.ELU(),\r\n",
    "                    nn.Linear(input_shape,input_shape),\r\n",
    "                    nn.ELU()             \r\n",
    "                ).to(device)           \r\n",
    "            )\r\n",
    "            \r\n",
    "    def forward(self,x):\r\n",
    "        \r\n",
    "        x_dsd_full = torch.Tensor(self.n_domains*self.batch_size,self.input_shape).to(self.device)\r\n",
    "        \r\n",
    "        for i in range(self.n_domains):\r\n",
    "            idxs = (i*self.batch_size,(i+1)*self.batch_size)\r\n",
    "            x_dsd = x[idxs[0]:idxs[1]] + self.Decoder_List[i].forward(x[idxs[0]:idxs[1]])\r\n",
    "            x_dsd_full[idxs[0]:idxs[1]] = x_dsd\r\n",
    "        \r\n",
    "        return x_dsd_full"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "class domain_style_adversaries(nn.Module):\r\n",
    "    def __init__(self,input_shape,n_domains,batch_size,device='cpu'):\r\n",
    "        super(domain_style_adversaries,self).__init__()\r\n",
    "        \r\n",
    "        self.input_shape = input_shape\r\n",
    "        self.n_domains = n_domains\r\n",
    "        self.batch_size = batch_size\r\n",
    "        self.device = device\r\n",
    "        \r\n",
    "        self.Adversary_List = nn.ModuleList().to(device)\r\n",
    "        \r\n",
    "        for i in range(n_domains):\r\n",
    "            self.Adversary_List.append(\r\n",
    "                nn.Sequential(\r\n",
    "                    #Add gradient reversal layer here?\r\n",
    "                    nn.Linear(input_shape,50),\r\n",
    "                    nn.ELU(),\r\n",
    "                    nn.Linear(50,1),\r\n",
    "                    nn.Sigmoid()\r\n",
    "                ).to(device)\r\n",
    "            )\r\n",
    "            \r\n",
    "    def forward(self,x):\r\n",
    "        y_dsa_full = torch.Tensor(self.n_domains*self.batch_size,1).to(self.device)\r\n",
    "        \r\n",
    "        for i in range(self.n_domains):\r\n",
    "            idxs = (i*self.batch_size,(i+1)*self.batch_size)\r\n",
    "            y_pred_dsa = self.Adversary_List[i].forward(x[idxs[0]:idxs[1]])\r\n",
    "            y_dsa_full[idxs[0]:idxs[1]] = y_pred_dsa\r\n",
    "        \r\n",
    "        return y_dsa_full"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class latent_domain_adversary(nn.Module):\r\n",
    "    def __init__(self,n_components,n_domains):\r\n",
    "        super(latent_domain_adversary,self).__init__()\r\n",
    "        \r\n",
    "        self.n_components = n_components\r\n",
    "        self.n_domains = n_domains\r\n",
    "        \r\n",
    "        self.Latent_Domain_Adversary = \r\n",
    "            nn.Sequential(\r\n",
    "                nn.Linear(n_components,n_domains*2),\r\n",
    "                nn.ELU(),\r\n",
    "                nn.Linear(n_domains*2,n_domains),\r\n",
    "                nn.Softmax()\r\n",
    "            )\r\n",
    "            \r\n",
    "    def forward(self,x):\r\n",
    "        return self.Latent_Domain_Adversary(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "class shared_encoder_decoder(nn.Module):\r\n",
    "    \r\n",
    "    def __init__(self,input_shape,n_components):\r\n",
    "        super(shared_encoder_decoder,self).__init__()\r\n",
    "        \r\n",
    "        self.se_hidden_1 = 100\r\n",
    "        self.n_components = 50\r\n",
    "        self.sd_hidden_1 = 100\r\n",
    "        self.n_components = n_components\r\n",
    "        \r\n",
    "        self.shared_encoder = nn.Sequential(\r\n",
    "            nn.Linear(input_shape,self.se_hidden_1),\r\n",
    "            nn.ELU(),\r\n",
    "            nn.Linear(self.se_hidden_1,self.n_components),\r\n",
    "            nn.ELU()\r\n",
    "        )\r\n",
    "        \r\n",
    "        self.se_mean = nn.Linear(self.n_components,self.n_components)\r\n",
    "        self.se_logvar = nn.Linear(self.n_components,self.n_components)\r\n",
    "        \r\n",
    "        self.shared_decoder = nn.Sequential(\r\n",
    "            nn.Linear(n_components,self.sd_hidden_1),\r\n",
    "            nn.ELU(),\r\n",
    "            nn.Linear(self.sd_hidden_1,input_shape),\r\n",
    "            nn.ELU()\r\n",
    "        )\r\n",
    "    def Sampling(self,mean,log_var):\r\n",
    "        eps = torch.randn(log_var.shape).to('cuda:0')\r\n",
    "        sample = mean + torch.exp(log_var/2)*eps\r\n",
    "        return sample\r\n",
    "    \r\n",
    "    def forward(self,x):\r\n",
    "        \r\n",
    "        x = self.shared_encoder(x)\r\n",
    "        mean = self.se_mean(x)\r\n",
    "        logvar = self.se_logvar(x)\r\n",
    "        z = self.Sampling(mean,logvar)\r\n",
    "        x_recon = self.shared_decoder(z)\r\n",
    "        \r\n",
    "        return (z,mean,logvar), x_recon\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "model = shared_encoder_decoder(784,n_components=25).to(device)\r\n",
    "lossFunction = nn.MSELoss()\r\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\r\n",
    "num_epochs = 5"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "for epoch in range(num_epochs):\r\n",
    "    loss_ = 0\r\n",
    "    for images,labels in train_loader:\r\n",
    "        images = images.reshape(-1,784).to(device)\r\n",
    "        dist_tuple,image_recon = model(images)\r\n",
    "        loss = lossFunction(image_recon,images)\r\n",
    "        optimizer.zero_grad()\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        \r\n",
    "        loss_ += loss.item()\r\n",
    "    print(\"Epoch{}, Training loss:{}\".format(epoch, loss_ / len(train_loader)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch0, Training loss:0.14060185118888816\n",
      "Epoch1, Training loss:0.08534790980940064\n",
      "Epoch2, Training loss:0.07328506947805484\n",
      "Epoch3, Training loss:0.06496107386114697\n",
      "Epoch4, Training loss:0.060066064186394215\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "idx=10\r\n",
    "plt.imshow(image_recon[idx].cpu().detach().numpy().reshape(28,28))\r\n",
    "plt.show()\r\n",
    "plt.imshow(images[idx].cpu().detach().numpy().reshape(28,28).astype(np.float32))\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "source": [
    "np.mean((image_recon[idx].cpu().detach().numpy().reshape(28,28) - images[idx].cpu().detach().numpy().reshape(28,28))**2)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.011492738"
      ]
     },
     "metadata": {},
     "execution_count": 151
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "source": [
    "np.mean(images[idx].cpu().detach().numpy().reshape(28,28))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.11883253"
      ]
     },
     "metadata": {},
     "execution_count": 145
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "source": [
    "np.mean(image_recon[idx].cpu().detach().numpy().reshape(28,28))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.112951905"
      ]
     },
     "metadata": {},
     "execution_count": 146
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}